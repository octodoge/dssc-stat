---
title: "GroupC_HW2"
author: "M. Polo, A. Campagnolo, S. D'Avenia, G. Cera"
date: '2022-11-10'
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 3

### Ex 3.12
*Simulate random sampling from a normal population distribution with several *$n$ *values to illustrate the law of large numbers.*

**Solution**  
We simulate three sets of data using a normal distribution $N(\mu=0, \ \sigma=10)$ with $n$ equal $10$, $100$ and $1000$.
```{r data_generation}
i <- 1:3
stand_dev <- 10
n <- 10^(i)
rn.x <- list()
for (j in i){
  rn.x[[j]] <- rnorm(n[j], sd = stand_dev)
}
```

We see that $mean \ of \ the \ data \rightarrow \mu = 0$ as $n$ increases.
```{r mean}
for (j in i){
  print(paste('n =', n[j], '  mean =', mean(rn.x[[j]])))
}
```

Displaying the sampled data in histograms and comparing them to the actual standard normal distribution we can see  that, as $n$ increases, the histogram better approximates the curve. 
```{r plots}
tr_colors <- c('lightblue', 'pink', 'lightgreen')

par(mfrow=c(2,2))
x <- -10:10
library(MASS)
for (j in i) {
  hist.scott(rn.x[[j]], col = tr_colors[j], xlim = range(-30,30), 
             ylim = range(0.,0.05), main = paste('n =', n[j]), xlab = 'x')
  curve(dnorm(x, sd = stand_dev), col = "red", type = 'l', lwd=2, add = T)
}
```



### Ex 3.18

*Sunshine City, which attracts primarily retired people, has 90,000 residents with a mean age of 72 years and a standard deviation of 12 years. The age distribution is skewed to the left. A random sample of 100 residents of Sunshine City has  $\bar{y} = 70$ and $s = 11$. a) Describe the center and spread of the (i) population distribution, (ii) sample data distribution. What shape does the sample data distribution probably have? Why? b)Find the center and spread of the sampling distribution of $\bar{y}$ for n = 100. What shape does it have and what does it describe? c) Explain why it would not be unusual to sample a person of age 60 in Sunshine City, but it would be highly unusual for the sample mean to be 60, for a random sample of 100 residents.*

**Solution**

#### a)

(i) The population distribution has mean 72, which we can take as a measure of center, while it has a standard deviation of 12 which we can take as a measure of spread. 

(ii) For the sample, the mean is 70 while the standard deviation is 11. Since the sample is random, we can assume that it is representative of the whole population, meaning that we can expect it will have the same distribution (more or less) as the population, i.e. a left skewed distribution. For this particular sample the mean and standard deviation are as stated above.


#### b) 

Under the reasonable assumption that a sample size of 100 is large enough for the Central Limit Theorem to hold we have that:
$$\bar{Y}_{100} \mathrel{\dot\sim} N(72, \frac{\sigma^2}{n} = 1.44)$$
Where with $\bar{Y}_{100}$ we denote the random variable describing the sample mean of a sample of size 100 collected from the population of Sunshine City.\
This means that this distribution is centered at 72 and has standard deviation 1.2. Under the CLT assumption this random variable has a normal distribution.


#### c)

Consider the first statement: "Sample a person of age 60 in Sunshine City" and consider $Y_i$ the random variable describing the age for a person in Sunshine City. We know that it is distributed with a mean of 72 and a standard deviation of 12, meaning that the specific value of 60 falls within 2 standard deviations of the sample mean, making it not that unlikely.

Now consider the second statement "Sample mean for a sample of size 100 is 60". We have seen in part b how the random variable $\bar{Y}_{100}$ has approximately the normal distribution with parameters stated above. Under these assumptions, the value of 60 falls exactly 10 standard deviations away from the mean, making it much less likely.



### Ex 3.28

*A survey is planned to estimate the population proportion $\pi$ supporting more government action to address global warming. For a simple random sample, if $\pi$ may be near $0.50$, how large should $n$ be so that the standard error of the sample proportion is $0.04 ?$*

**Solution**

The standard error of the sampling distribution of the sample proportion $\bar{\pi}$ is 
$$\sigma_{\hat{\pi}}=\sqrt{\frac{\pi(1-\pi)}{n}}$$
If half of the population (0.50) are supporting the government action to address globar warming, for a simple random sample $n$ should be
$$
(0.04)=\sqrt{\frac{0.50(1-0.50)}{n}} \Rightarrow (0.04)=\sqrt{\frac{0.25}{n}} \Rightarrow (0.04)^2=\frac{0.25}{n} \Rightarrow \frac{0.04^2}{0.25}=\frac{1}{n} \Rightarrow n=\frac{0.25}{0.0016}=156,25
$$



## FSDS - Chapter 4 

### Ex 4.2

*For a sequence of observations of a binary random variable, you observe the geometric random variable (Section 2.2.2) outcome of the first success on observation number* $y=3$*. Find and plot the likelihood function.*

**Solution**

Since we observe the first success at the third trial, we have the following observations of a Bernoulli r.v.:
$$
0, 0, 1
$$
The likelihood function is the joint probability of our observations viewed as a function of the parameter:
$$
L(p) = (1-p)^2 p
$$
Now let's plot it:

```{r}
L = function(p) {
  (1-p)^2 * p
}
curve(L, xlab='p', ylab='L(p)') #p already in [0,1] by default
```

### Ex 4.4

*For the Students data file (Exercise 1.2 in Chapter 1) and corresponding population, find the ML estimate of the population proportion believing in life after death. Construct a Wald 95% confidence interval, using its formula (4.8). Interpret.*

**Solution**
We only need to consider the life variable in the Student dataset, which is recorded as: (1 = yes, 2 = no, 3 = undecided).
Assuming that the student responses are independent, we can model the responses as a set of i.i.d. Bernoulli distribution with parameter $\pi$. This can be stated as 
$$Y_i \sim i.i.d. \  Ber(\pi) \qquad i = 1, ... 60$$
The ML estimator for this model is given by the sample mean $$\hat{\pi} = \frac{\sum_{i=0}^{60}y_i}{n} $$.

To construct an approximate Wald 95% Confidence interval an approximate pivot is:
$$Z(\pi) = \frac{\bar{y} - \pi}{\sqrt{\frac{\bar{y}(1-\bar{y})}{n}}} \qquad \bar{y} = \hat{\pi}$$
So a 95% Wald confidence interval for $\pi$ is given by:
$$\bar{y}\pm z_{1-0.05/2}\sqrt{\frac{\bar{y}(1-\bar{y})}{n}}$$
```{r Students}
data <- read.table("http://stat4ds.rwth-aachen.de/data/Students.dat", header = TRUE)
life <- data$life

# 1 if students believes in life after death, 0 otherwise
y <- as.numeric(life==1)
n <- length(y)
pi_hat <- sum(y)/n

#Now we construct the CI
alpha <- 0.05
z_alpha <- qnorm(1 - alpha/2)
se <- sqrt( (pi_hat * (1-pi_hat))/n)
lower <- pi_hat + z_alpha * se
upper <- pi_hat - z_alpha * se


```
The ML estimate is `r round(pi_hat, 4)`, while the Wald Confidence Interval is [`r round(lower, 4)`; `r round(upper, 4)` ]

### Ex 4.38

*For independent observations $y_1, \ldots, y_n$ having the geometric distribution (2.1):*
*(a) Find a sufficient statistic for $\pi$.*
*(b) Derive the ML estimator of $\pi$. *

**Solution**

a)
The joint probability mass function of $y_1,...,y_n$ is:
$$
f(y_1,...,y_n|\pi) = \prod_{i=1}^{n}(1-\pi)^{y_i}\pi = \pi^n \prod_{i=1}^{n}(1-\pi)^{y_i} = \pi^n (1-\pi)^{y_1+...+y_i} = \pi^n (1-\pi)^{\sum_{i=1}^{n}y_i}
$$
Set $T(y) = \sum_{i=1}^{n} y_i$, we see that $f(y|\pi)$ has been expressed as a function that depends on $\pi$ but depends on the observations y only through the value of $\sum_{i=1}^{n} y_i$
By the factorization theorem, $T(y) = \sum_{i=1}^{n} y_i$ is a sufficient statistic.

b)

The log likelihood function is defined as

$$
\ell(\theta) = \sum_{i=1}^{n}log[(1-\theta)^{y_i-1}\theta] = \sum_{i=1}^{n}[log(1-\theta)^{y_i-1}+log(\theta)] = log(1-\theta)\sum_{i=1}^{n}({y_i-1})+n\,log(\theta) = log(1-\theta)(\sum_{i=1}^{n}{(y_i)-n})+n\,log(\theta)
$$
The likelihood equation for estimating $\theta$ is
$$
\frac{d}{d\theta}log\, L = 0 \Rightarrow \frac{-1}{1-\theta} (\sum_{i=1}^{n}{(y_i)-n})+\frac{n}{\theta}=0 \Rightarrow \frac{n}{\theta} =  \frac{\sum_{i=1}^{n}{(y_i)-n}}{1-\theta} \Rightarrow n-n\theta = \theta\sum_{i=1}^{n}{(y_i)-n\theta} \Rightarrow \theta=\frac{n}{\sum_{i=1}^{n}{(y_i)}} \Rightarrow \theta=\frac{1}{\bar{y}}
$$
So MLE of $\theta$ is $\frac{1}{\bar{y}}$.


### Ex 4.44

*Refer to the previous two exercises. Consider the selling prices (in thousands of dollars) in the* **`Houses`** *data file mentioned in Exercise 4.31.*

*(a) Fit the normal distribution to the data by finding the ML estimates of $\mu$ and $\sigma$ for that distribution.*  
*(b) Fit the log-normal distribution to the data by finding the ML estimates of its parameters.*  
*(c) Find and compare the ML estimates of the mean and standard deviation of selling price for the two distributions.*  
*(d) Superimpose the fitted normal and log-normal distributions on a histogram of the data. Which distribution seems to be more appropriate for summarizing the selling prices?*

**Solution**

```{r import_dataset}
# import dataset
Houses <- read.table('http://stat4ds.rwth-aachen.de/data/Houses.dat', 
                     header = T)
```

**(a)** Likelihood function for **normal distribution**:
$$\begin{aligned}
\ell(\bf{y},\mu,\sigma)
&=\prod_{i=1}^nf(\bf{y}, \mu, \sigma)\\
&=\prod_{i=1}^n(2\pi\sigma^2)^{-\frac{1}{2}}\exp\left(-\frac{1}{2}\frac{(y_i-\mu)^2}{\sigma^2}\right)\\
&=(2\pi\sigma^2)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2\right)
\end{aligned}$$
Log Likelihood function for normal distribution:
$$\begin{aligned}
L(\bf{y}, \mu, \sigma)
&=\ln(\ell(\bf{y},\mu,\sigma))\\
&=\ln\left((2\pi\sigma^2)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2\right)\right)\\
&=\ln\left((2\pi\sigma^2)^{-\frac{n}{2}}\right)+\ln\left(\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2\right)\right)\\
&=-\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2\\
&=-\frac{n}{2}\left(\ln(2\pi)+\ln(\sigma^2)\right)-\frac{1}{2\sigma^2}\sum_{j=1}^n(y_i-\mu)^2
\end{aligned}$$
Maximum Likelihood estimator:  
we need to solve the following $$\max_{\mu,\sigma}L(\bf{y},\mu,\sigma)$$  
Differentiating for $\mu$ and $\sigma$ to find the likelihood equation for each parameter 
$$\frac{\partial L(\bf{y},\mu,\sigma)}{\partial\mu}=0$$ 
$$\frac{\partial L(\bf{y},\mu,\sigma)}{\partial\sigma}=0$$
The partial derivative of the log-likelihood function in regards to the **mean** is the following
$$\begin{aligned}
\frac{\partial L(\bf{y},\mu,\sigma)}{\partial\mu}
&=\frac{\partial}{\partial\mu}\left(-\frac{n}{2}\left(\ln(2\pi)+\ln(\sigma^2)\right)-\frac{1}{2\sigma^2}\sum_{j=1}^n(y_i-\mu)^2\right)\\
&=\frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\mu)\\
&=\frac{1}{\sigma^2}\left(\sum_{i=1}^n y_i-n\mu\right)
\end{aligned}$$
setting this equal to zero means the numerator has to be zero $$\sum_{i=1}^n y_i-n\mu=0$$
therefore the result is $$\mu=\frac{1}{n}\sum_{i=1}^{n}y_i$$

The partial derivative of the log-likelihood function in regards to the **standard deviation** is the following
$$\begin{aligned}
\frac{\partial L(\bf{y},\mu,\sigma)}{\partial\sigma}
&=\frac{\partial}{\partial\sigma}\left(-\frac{n}{2}\left(\ln(2\pi)+\ln(\sigma^2)\right)-\frac{1}{2\sigma^2}\sum_{j=1}^n(y_i-\mu)^2\right)\\
&=-\frac{n}{\sigma}+\frac{1}{\sigma^3}\sum_{i=1}^{n}(y_i-\mu)^2\\
&=\frac{1}{\sigma}\left(\frac{1}{\sigma^2}\sum_{i=1}^{n}(y_i-\mu)^2-n\right)
\end{aligned}$$
assuming $\sigma\ne0$, the expression is equal to zero only if $$\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\mu)^2}$$
In conclusion the two likelihood equations are
$$\begin{aligned}
&\hat\mu=\frac{1}{n}\sum_{i=1}^{n}y_i\\
&\hat\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat\mu)^2}
\end{aligned}$$
which are respectively, the sample mean and (elevating to the square to get the variance from the standard deviation), the unadjusted sample variance.  

Translating these in R we can get estimates on the houses size.
```{r norm_estimates}
y <- Houses$price
n <- length(Houses$size)
mu <- sum(y)/n
sig <- sqrt(sum((y-mu)^2)/n)
print(paste("mu_hat =", mu))
print(paste("sigma_hat =", sig))
```

**(b)**
The procedure is the same for the **log-normal distribution** but we can use the function `fitdistr()` from the `MASS` library to avoid all the math and just get the estimates for the two parameters.
```{r log_norm_estimates}
# y: argument defined for the norm MLE
parameters <- MASS::fitdistr(y, densfun = 'log-normal')
parameters
```
 
**(c)** 
We apply `fitdistr()` again on the prices for both the normal and log-normal distribution.
```{r price_MLE}
y <- Houses$price
n <- length(Houses$price)
# Normal distribution
MLE_norm <- MASS::fitdistr(y, densfun = 'normal')
# Log-Normal distribution
MLE_logn <- MASS::fitdistr(y, densfun = 'log-normal')

MLE_norm # Normal distribution 
MLE_logn # Log-Normal distribution parameters mu and sigma
```
For the log-normal distribution the mean is given by $exp(\mu + \frac{\sigma^2}{2})$ while the variance is $[exp(\sigma^2)-1]exp(2\mu + \sigma^2)$. We can compute these with the parameters we have found above:
```{r price_MLE2}
mean_lognormal <- exp(MLE_logn$estimate[1] + MLE_logn$estimate[2]^2/2)
sd_lognormal <- sqrt((exp(MLE_logn$estimate[2]^2) - 1)*exp(2 * MLE_logn$estimate[1] + MLE_logn$estimate[2]^2))
cbind(mean_lognormal , sd_lognormal)
```
We can see that the estimates for the mean are very similar, while those for the standard deviation are slightly different.

**(d)** 
```{r}
# data
x <- Houses$price 
# Histogram of data density
MASS::hist.scott(x, col = 'aquamarine', xlab = 'Prices', main='House prices')
# plot of fitted normal distribution
curve(dnorm(x, mean=MLE_norm$estimate[1], sd=MLE_norm$estimate[2]), 
       col='purple', lwd=4, add=T)
# plot of fitted log-normal distribution
curve(dlnorm(x, mean=MLE_logn$estimate[1], sd=MLE_logn$estimate[2]), 
       col='orange', lwd=4, add=T)
# graph legend
legend(x='topright', 
       legend=c('real data','normal distribution','log-normal distribution'), 
       col=c('aquamarine','purple','orange'),
       pch=c(15,19,19))
```  

The log normal distribution seems to represent better the data since it is not symmetrical.


### Ex 4.48

*For a simple random sample of* $n$ *subjects, explain why it is about 95% likely that the sample proportion has error no more than* $1/\sqrt{n}$ *in estimating the population proportion. (Hint: To show this "*$1/\sqrt{n}$ *rule", find two standard errors when* $\pi = 0.50$*, and explain how this compares to two standard errors at other values of* $\pi$*.) Using this result, show that* $n = 1/M^2$ *is a safe sample size for estimating a proportion to within* $M$ *with 95% confidence.*

**Solution**

The sample proportion can be seen as the sample mean of a Bernoulli random sample with parameter $\pi$. Therefore, for the Central Limit Theorem, it is approximately normally distributed with mean $\pi$ and standard deviation:
$$
\sqrt{(1-\pi)\pi} \over \sqrt{n}
$$
This is also the standard error of the sample proportion because the standard error of a statistic is the standard deviation of its sampling distribution (or an estimate of that standard deviation).

For a normal distribution, about 95% of values fall within 2 standard deviations of the mean. This means that, in estimating the population proportion, it is about 95% likely that the sample proportion has error no more than:
$$
2 {\sqrt{(1-\pi)\pi} \over \sqrt{n}}
$$
However, note that $(1-\pi)\pi$ is maximized by $\pi = 0.5$. So, actually, the error is no more than:
$$
2 {\sqrt{(1-0.5)0.5} \over \sqrt{n}} = {1 \over \sqrt{n}}
$$
Using this result, we can find a safe sample size $n$ for estimating a proportion within $M$ with 95% confidence:
$$
M = {1 \over \sqrt{n}} \implies n = {1 \over M^2}
$$

## FSDS - Chapter 5 

### Ex 5.2

*When a government does not have enough money to pay for the services that it provides, it can raise taxes or it can reduce services. When the Florida Poll asked a random sample of 1200 Floridians which they preferred, 52% (624 of the 1200) chose raise taxes and 48% chose reduce services. Let $\pi$ denote the population proportion of Floridians who would choose raising taxes. Analyze whether this is a minority of the population ($\pi <$ 0.50) or a majority ($\pi >$ 0.50) by testing H0 ∶ $\pi$ = 0.50 against Ha: $\pi \neq $ 0.50. Interpret the P-value. Is it appropriate to “accept H0? Why or why not?*

**Solution**
We denote as $\pi$ the proportion of Floridians who would choose to raise taxes. We have the following set of hypotheses:
$$H_0: \pi = 0.5 \qquad H_1: \pi \neq 0.5$$
Since the sample size is quite large, the sampling distribution of $\hat{\pi}$, which is the proportion of people in the sample saying they prefer taxes to be raised, is approximately normal (by CLT), and under $H_0$ we have that its distribution is as follows
$$\hat{\pi} \mathrel{\dot\sim} N(\pi_0,\frac{\pi_0 (1-\pi_0)}{n}) = N(0.5, \frac{0.25}{1200}) \ \ (\text{In this case}) $$
Hence we can obtain the p-value for this set of hypotheses as the probability of observing a value as "extreme" or more than the observed 0.52 in both direction (since we are considering a double sided test).

```{r Florida_Poll}
# Code as 1 those who prefer raising taxes
y <- c(rep(1, 624), rep(0, 1200-624))
n <- length(y)

pi_hat <- sum(y)/n
se <- sqrt(0.5 * (1 - 0.5)/n)
test <- (pi_hat - 0.5)/se

p <- 2*pnorm(test, lower.tail = FALSE)
```
The p value we obtain is `r round(p, 2)`. This p-value is not very strong, meaning that we do not have enough evidence supporting the hypothesis that the proportion of voters is different than the 0.5 stated in the null hypothesis. Hence I would reject the alternative hypothesis. \
In any case it would be wrong to state that we "accept the null hypothesis", as we assume that the null hypothesis is true and work under that construct. We are simply trying to see if there is enough evidence against it to support an alternative hypothesis.


### Ex 5.12

*The example in Section 3.1.4 described an experiment to estimate the mean sales with a proposed menu for a new restaurant. In a revised experiment to compare two menus, on Tuesday of the opening week the owner gives customers menu A and on Wednesday she gives them menu B. The bills average \$22.30 for the 43 customers on Tuesday (s = 6.88) and \$25.91 for the 50 customers on Wednesday (s = 8.01). Under the strong assumption that her customers each night are comparable to a random sample from the conceptual population of potential customers, show how to compare the mean sales for the two menus based on (a) the P -value of a significance test, (b) a 95% confidence interval. Which is more informative, and why? (When used in an experiment to compare two treatments to determine which works better, a two-sample test is often called an A/B test.).*

**Solution**

a)
We test $H0: \mu_2 = \mu_1$ against $Ha: \mu_2 > \mu_1$

The pooled standard deviation estimate is:
$$
s=\sqrt{\frac{\left(n_1-1\right) s_1^2+\left(n_2-1\right) s_2^2}{n_1+n_2-2}}= \sqrt{\frac{\left(43-1\right) 6.88^2+\left(50-1\right) 8.01^2}{43+50-2}} = \sqrt{\frac{5131.8897}{91}} = 7.509...
$$
and $\bar{y_2} − \bar{y_1} =  25.91 -22.30  = 3.61$ has estimated standard error:
$$
s e=s \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}=7.51 \sqrt{\frac{1}{43}+\frac{1}{50}}= 1.54
$$
For testing $H0: \mu_1 = \mu_2$, the observed value of the $T$ test statistic is
$$
t=\frac{\bar{y}_2-\bar{y}_1}{s e}=\frac{25.91 -22.30}{1.54}=2.34,
$$
with $df = n_1 +n_2 −2 = 43+50-2=91$.
A two sided test is performed:
```{r}
#set.seed(7)
#menuA <- c(rnorm(43, mean = 22.30, sd = 6.88))
#menuB <- c(rnorm(50, mean = 25.91, sd = 8.01))
#t.test(menuB, menuA, var.equal = TRUE, alternative = "greater")
p_value=2*pt(q=2.34, df=91, lower.tail = FALSE)
p_value
```
give a $P$-value of 0.0212.

b)
From the previous answer, we know that $s=7.51$, $\bar{y_2} − \bar{y_1} = 3.61$ and $se=1.54$.

With $df =n_1 +n_2 −2=91$, a 95% confidence interval for $(\mu_1 −\mu_2)$ is calculated
```{r}
sample.se = 1.54
sample.mean = 3.61
t.score = qt(p=.95, df=91,lower.tail=T)
t.score

margin.error <- t.score * sample.se
margin.error
lower.bound <- sample.mean - margin.error
upper.bound <- sample.mean + margin.error
print(c(lower.bound,upper.bound))
```

$$
\left(\bar{y}_2-\bar{y}_1\right) \pm t_{0.95,91}(s e)=3.61 \pm 1.66(1.54) \text {, which is } 3.61 \pm 2.56 \text {, or }(1.05, 6.17) \text {. }
$$

To compare the mean sales, the p-value is more informative since allow a decision about the rejection or maintenance of the previously formulated null hypothesis. In this case we reject $H_0$ because P-value of 0.011 is less than 0.05 and we can infer that $\mu_2>\mu_1$, so menu B works better than menu A.


### Ex 5.16

*An experiment used a sample of college students to investigate whether cell phone use impairs drivers’ reaction times. On a machine that simulated driving situations, at irregular periods a target flashed red or green. Participants were instructed to press a brake button as soon as possible when they detected a red light. Under the cell phone condition, each student carried out a conversation on a cell phone with someone in a separate room. In the control condition, the same students listened to a radio broadcast. The "CellPhone" data file records the students’ mean response times (in milliseconds) over several trials for each condition,* $\{y_{i1}\}$ *for the cell phone condition and* $\{y_{i2}\}$ *for control.*

a. *The comparisons of means or proportions in this chapter assume independent samples for the two groups. Explain why the samples for these two conditions are dependent rather than independent.*

b. *To compare* $\mu_1$ *and* $\mu_2$*, you can use* $\{d_i = y_{i1} − y_{i2},\; i=1,...,n\}$*, here with* $n=8$*. Specify the parameter* $\mu_d$ *and* $H_0$ *for doing this, and explain why* $\mu_d = \mu_1 - \mu_2$*.*

c. *State the assumptions and test statistic, explain why it has a t-distribution with* $df = n−1$*. Report the P-value with two-sided* $H_a$*, and interpret. (The test is called a matched-pairs t-test. Matched-pairs analyses also are possible with confidence intervals, as Section 4.4.3 did in comparing weights of anorexic girls before and after a period of treatment by analyzing the mean difference in weights.)*

**Solution**

#### a.

The samples for these two conditions are dependent because the students are the same.

#### b.

Let $\mu_d$ be the population mean of the differences $d_i$. To compare the population means $\mu_1$ and $\mu_2$ of the two conditions, we can test the null hypothesis $H_0: \mu_d = 0$. In fact $\mu_d = \mu_1 - \mu_2$:
$$
\mu_d = {1 \over m} \sum_{i=1}^m d_i = {1 \over m} \sum_{i=1}^m (y_{i1} − y_{i2}) =
{1 \over m} \sum_{i=1}^m y_{i1} - {1 \over m} \sum_{i=1}^m y_{i2} = \mu_1 - \mu_2
$$

#### c.

Now we want to do a significance test for the mean $\mu_d$.

We assume that the differences $\{d_i = y_{i1} − y_{i2},\; i=1,...,8\}$ are independent. They are also approximately normally distributed, since the mean response times $y_{i1}$ and $y_{i2}$ are approximately normally distributed for the Central Limit Theorem (since they are means). But, actually, the significance test for the mean is quite robust and thus performs adequately even if this last assumption is violated.

For this test, the test statistic is:
$$
T = {\bar Y - \mu_d \over {s \over \sqrt n}}
$$
with $\mu_d = 0$ because we assume that the null hypothesis is true.

This test statistic has a t-distribution with $df = n−1 = 7$ (instead of a standard normal distribution) because it uses the sample standard deviation $s$ (instead of the population standard deviation $\sigma$, which is unknown).

Let's explore the data file with the mean response times $y_{i1}$ and $y_{i2}$:

```{r}
data = read.table("http://stat4ds.rwth-aachen.de/data/CellPhone.dat", header=TRUE)
data
```

Let's calculate the observation of the test statistic $T$ with the formula above:

```{r}
diffs = data$phone - data$control #differences
T_obs = mean(diffs) * sqrt(8) / sd(diffs) #observation of T
T_obs
```

Finally, let's calculate the two-sided P-value:

```{r}
2 * pt(T_obs, df=7, lower.tail=FALSE) #2 times because two-sided
```

The P-value is the probability that the test statistic equals the observed value (or a more extreme one) assuming that $H_0$ is true. In this case it is quite low, meaning that we can confidently reject $H_0$ in favor of the alternative hypothesis $H_a: \mu_d \ne 0$. This indicates that cell phone use impairs drivers' reaction times.

### Ex 5.68

*Explain why the confidence interval based on the Wald test of $H_0:\theta=\theta_0$ is symmetric around $\hat\theta$ (i.e., having center exactly equal to $\hat\theta$. This is not true for the confidence intervals based on the likelihood-ratio and score tests.) Explain why such symmetry can be problematic when $\theta$ and $\hat\theta$ are near a boundary, using the example of a population proportion that is very close to 0 or 1 and a sample proportion that may well equal 0 or 1.*

**Solution**  
Wald confidence interval $$\hat\theta\pm z_{\alpha/2}\sqrt{\frac{\hat\theta(1-\hat\theta)}{n}}$$
It is symmetrical around $\hat\theta$ by construction.  
Wald test $$ W=\frac{(\hat\theta-\theta_0)^2}{SE(\hat\theta)^2} $$
To demonstrate that the Wald test does not perform well near boundaries we use the function `binom.coverage` from the package `binom`, which determines the probability coverage for a binomial confidence interval.
```{r}
library(binom)
# n=30  p=0.1
binom.coverage(0.1, 30, conf.level = 0.95, method="asymptotic")
# Wald CI has true probability = 0.81 of containing pi = 0.1
```
We can see that coverage is lower than the requested confidence level of `0.95`. The reason for this discrepancy is that the confidence interval, being symmetric, falls outside the parameter space for the proportion, resulting in an empirical coverage lower than the thoretical one.


