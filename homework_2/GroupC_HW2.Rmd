---
title: "GroupC_HW2"
author: "M. Polo, A. Campagnolo, S. D'Avenia, G. Cera"
date: '2022-11-10'
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 3

### Ex 3.12
*Simulate random sampling from a normal population distribution with several *$n$ *values to illustrate the law of large numbers.*

**Solution**  
We simulate three sets of data using a normal distribution $\sim N(\mu=0, \ \sigma=10)$ with $n$ equal $10$, $100$ and $1000$.
```{r data_generation}
i <- 1:3
stand_dev <- 10
n <- 10^(i)
rn.x <- list((1:n[1]), (1:n[2]), (1:n[3]))
for (j in i){
  rn.x[[j]] <- rnorm(n[j], sd = stand_dev)
}
```

We see that $mean \ of \ the \ data \rightarrow \mu = 0$ as $n$ increases.
```{r mean}
for (j in i){
  print(paste('n =', n[j], '  mean =', mean(as.numeric(unlist(rn.x[j])))))
}
```

Following the definition for the Law of large numbers in Agresti-Kateri's book under section *3.2.5*, $\mu = 0$, we can set a value for $\epsilon = 2$ and calculate $P(|\overline{Y}-\mu|\geq\epsilon)\rightarrow0$ simply by counting the number of values generated that are further away from $\mu$ than $\epsilon$ and dividing it by the total number of values generated, $n$.
```{r}
eps <- 2                            # epsilon
for (j in i){
  v <- as.numeric(unlist(rn.x[j]))
  under_eps <- v[abs(v) >= eps]     # values further away from mu than epsilon
  print(paste('n =', n[j], '  P =', length(under_eps)/n[j]))
}
```
$P$ tends to decrease as $n$ increases.

Displaying the sampled data in histograms and comparing them to the actual standard normal distribution we can see  that, as $n$ increases, the histogram better approximates the curve. 
```{r plots}
tr_colors <- c('lightblue', 'pink', 'lightgreen')

par(mfrow=c(2,2))
x <- -10:10
library(MASS)
for (j in i) {
  hist.scott(as.numeric(unlist(rn.x[j])), col = tr_colors[j], xlim = range(-30,30), 
             ylim = range(0.,0.05), main = paste('n =', n[j]), xlab = 'x')
  curve(dnorm(x, sd = stand_dev), col = "red", type = 'l', lwd=2, add = T)
}
```



### Ex 3.18

*Sunshine City, which attracts primarily retired people, has 90,000 residents with a mean age of 72 years and a standard deviation of 12 years. The age distribution is skewed to the left. A random sample of 100 residents of Sunshine City has  $\bar{y} = 70$ and $s = 11$. a) Describe the center and spread of the (i) population distribution, (ii) sample data distribution. What shape does the sample data distribution probably have? Why? b)Find the center and spread of the sampling distribution of $\bar{y}$ for n = 100. What shape does it have and what does it describe? c) Explain why it would not be unusual to sample a person of age 60 in Sunshine City, but it would be highly unusual for the sample mean to be 60, for a random sample of 100 residents.*

**Solution**

#### a)

(i) The population distribution has mean 72, which we can take as a measure of center, while it has a standard deviation of 12 which we can take as a measure of spread. 

(ii) For the sample, the mean is 70 while the standard deviation is 11. Since the sample is random, we can assume that it is representative of the whole population, meaning that we can expect it will have the same distribution (more or less) as the population, i.e. a left skewed distribution. For this particular sample the mean and standard deviation are as stated above.


#### b) 

Under the reasonable assumption that a sample size of 100 is large enough for the Central Limit Theorem to hold we have that:
$$\bar{Y}_{100} \mathrel{\dot\sim} N(72, \frac{\sigma^2}{n} = 1.44)$$
Where with $\bar{Y}_{100}$ we denote the random variable describing the sample mean of a sample of size 100 collected from the population of Sunshine City.\
This means that this distribution is centered at 72 and has standard deviation 1.2. Under the CLT assumption this random variable has a normal distribution.


#### c)

Consider the first statement: "Sample a person of age 60 in Sunshine City" and consider $Y_i$ the random variable describing the age for a person in Sunshine City. We know that it is distributed with a mean of 72 and a standard deviation of 12, meaning that the specific value of 60 falls within 2 standard deviations of the sample mean, making it not that unlikely.

Now consider the second statement "Sample mean for a sample of size 100 is 60". We have seen in part b how the random variable $\bar{Y}_{100}$ has approximately the normal distribution with parameters stated above. Under these assumptions, the value of 60 falls exactly 10 standard deviations away from the mean, making it much less likely.



### Ex 3.28

*Here the text of the second exercise.*

**Solution**

Add comments to the solution.



## FSDS - Chapter 4 

### Ex 4.2

*For a sequence of observations of a binary random variable, you observe the geometric random variable (Section 2.2.2) outcome of the first success on observation number* $y=3$*. Find and plot the likelihood function.*

**Solution**

Since we observe the first success at the third trial, we have the following observations of a Bernoulli r.v.:
$$
0, 0, 1
$$
The likelihood function is the joint probability of our observations viewed as a function of the parameter:
$$
L(p) = (1-p)^2 p
$$
Now let's plot it:

```{r}
L = function(p) {
  (1-p)^2 * p
}
curve(L, xlab='p', ylab='L(p)') #p already in [0,1] by default
```

### Ex 4.4

*For the Students data file (Exercise 1.2 in Chapter 1) and corresponding population, find the ML estimate of the population proportion believing in life after death. Construct a Wald 95% confidence interval, using its formula (4.8). Interpret.*

**Solution**
We only need to consider the life variable in the Student dataset, which is recorded as: (1 = yes, 2 = no, 3 = undecided).
Assuming that the student responses are independent, we can model the responses as a set of i.i.d. Bernoulli distribution with parameter $\pi$. This can be stated as 
$$Y_i \sim i.i.d. \  Ber(\pi) \qquad i = 1, ... 60$$
The ML estimator for this model is given by the sample mean $$\hat{\pi} = \frac{\sum_{i=0}^{60}y_i}{n} $$.

To construct an approximate Wald 95% Confidence interval an approximate pivot is:
$$Z(\pi) = \frac{\bar{y} - \pi}{\sqrt{\frac{\bar{y}(1-\bar{y})}{n}}} \qquad \bar{y} = \hat{\pi}$$
So a 95% Wald confidence interval for $\pi$ is given by:
$$\bar{y}\pm z_{1-0.05/2}\sqrt{\frac{\bar{y}(1-\bar{y})}{n}}$$
```{r Students}
data <- read.table("http://stat4ds.rwth-aachen.de/data/Students.dat", header = TRUE)
life <- data$life

# 1 if students believes in life after death, 0 otherwise
y <- as.numeric(life==1)
n <- length(y)
pi_hat <- sum(y)/n

#Now we construct the CI
alpha <- 0.05
z_alpha <- qnorm(1 - alpha/2)
se <- sqrt( (pi_hat * (1-pi_hat))/n)
lower <- pi_hat + z_alpha * se
upper <- pi_hat - z_alpha * se


```
The ML estimate is `r round(pi_hat, 4)`, while the Wald Confidence Interval is [`r round(lower, 4)`; `r round(upper, 4)` ]

### Ex 4.38

*Here the text of the second exercise.*

**Solution**

Add comments to the solution.


### Ex 4.44

*Here the text of the second exercise.*

**Solution**

Add comments to the solution.


### Ex 4.48

*For a simple random sample of* $n$ *subjects, explain why it is about 95% likely that the sample proportion has error no more than* $1/\sqrt{n}$ *in estimating the population proportion. (Hint: To show this "*$1/\sqrt{n}$ *rule", find two standard errors when* $\pi = 0.50$*, and explain how this compares to two standard errors at other values of* $\pi$*.) Using this result, show that* $n = 1/M^2$ *is a safe sample size for estimating a proportion to within* $M$ *with 95% confidence.*

**Solution**

The sample proportion can be seen as the sample mean of a Bernoulli random sample with parameter $\pi$. Therefore, for the Central Limit Theorem, it is approximately normally distributed with mean $\pi$ and standard deviation:
$$
\sqrt{(1-\pi)\pi} \over \sqrt{n}
$$
This is also the standard error of the sample proportion because the standard error of a statistic is the standard deviation of its sampling distribution (or an estimate of that standard deviation).

For a normal distribution, about 95% of values fall within 2 standard deviations of the mean. This means that, in estimating the population proportion, it is about 95% likely that the sample proportion has error no more than:
$$
2 {\sqrt{(1-\pi)\pi} \over \sqrt{n}}
$$
However, note that $(1-\pi)\pi$ is maximized by $\pi = 0.5$. So, actually, the error is no more than:
$$
2 {\sqrt{(1-0.5)0.5} \over \sqrt{n}} = {1 \over \sqrt{n}}
$$
Using this result, we can find a safe sample size $n$ for estimating a proportion within $M$ with 95% confidence:
$$
M = {1 \over \sqrt{n}} \implies n = {1 \over M^2}
$$

## FSDS - Chapter 5 

### Ex 5.2

*When a government does not have enough money to pay for the services that it provides, it can raise taxes or it can reduce services. When the Florida Poll asked a random sample of 1200 Floridians which they preferred, 52% (624 of the 1200) chose raise taxes and 48% chose reduce services. Let $\pi$ denote the population proportion of Floridians who would choose raising taxes. Analyze whether this is a minority of the population ($\pi <$ 0.50) or a majority ($\pi >$ 0.50) by testing H0 ∶ $\pi$ = 0.50 against Ha: $\pi \neq $ 0.50. Interpret the P-value. Is it appropriate to “accept H0? Why or why not?*

**Solution**
We denote as $\pi$ the proportion of Floridians who would choose to raise taxes. We have the following set of hypotheses:
$$H_0: \pi = 0.5 \qquad H_1: \pi \neq 0.5$$
Since the sample size is quite large, the sampling distribution of $\hat{\pi}$, which is the proportion of people in the sample saying they prefer taxes to be raised, is approximately normal (by CLT), and under $H_0$ we have that its distribution is as follows
$$\hat{\pi} \mathrel{\dot\sim} N(\pi_0,\frac{\pi_0 (1-\pi_0)}{n}) = N(0.5, \frac{0.25}{1200}) \ \ (\text{In this case}) $$
Hence we can obtain the p-value for this set of hypotheses as the probability of observing a value as "extreme" or more than the observed 0.52 in both direction (since we are considering a double sided test).

```{r Florida_Poll}
# Code as 1 those who prefer raising taxes
y <- c(rep(1, 624), rep(0, 1200-624))
n <- length(y)

pi_hat <- sum(y)/n
se <- sqrt(0.5 * (1 - 0.5)/n)
test <- (pi_hat - 0.5)/se

p <- 2*pnorm(test, lower.tail = FALSE)
```
The p value we obtain is `r round(p, 2)`. This p-value is not very strong, meaning that we do not have enough evidence supporting the hypothesis that the proportion of voters is different than the 0.5 stated in the null hypothesis. Hence I would reject the alternative hypothesis. \
In any case it would be wrong to state that we "accept the null hypothesis", as we assume that the null hypothesis is true and work under that construct. We are simply trying to see if there is enough evidence against it to support an alternative hypothesis.


### Ex 5.12

*Here the text of the second exercise.*

**Solution**

Add comments to the solution.

### Ex 5.16

*An experiment used a sample of college students to investigate whether cell phone use impairs drivers’ reaction times. On a machine that simulated driving situations, at irregular periods a target flashed red or green. Participants were instructed to press a brake button as soon as possible when they detected a red light. Under the cell phone condition, each student carried out a conversation on a cell phone with someone in a separate room. In the control condition, the same students listened to a radio broadcast. The "CellPhone" data file records the students’ mean response times (in milliseconds) over several trials for each condition,* $\{y_{i1}\}$ *for the cell phone condition and* $\{y_{i2}\}$ *for control.*

a. *The comparisons of means or proportions in this chapter assume independent samples for the two groups. Explain why the samples for these two conditions are dependent rather than independent.*

b. *To compare* $\mu_1$ *and* $\mu_2$*, you can use* $\{d_i = y_{i1} − y_{i2},\; i=1,...,n\}$*, here with* $n=8$*. Specify the parameter* $\mu_d$ *and* $H_0$ *for doing this, and explain why* $\mu_d = \mu_1 - \mu_2$*.*

c. *State the assumptions and test statistic, explain why it has a t-distribution with* $df = n−1$*. Report the P-value with two-sided* $H_a$*, and interpret. (The test is called a matched-pairs t-test. Matched-pairs analyses also are possible with confidence intervals, as Section 4.4.3 did in comparing weights of anorexic girls before and after a period of treatment by analyzing the mean difference in weights.)*

**Solution**

#### a.

The samples for these two conditions are dependent because the students are the same.

#### b.

Let $\mu_d$ be the population mean of the differences $d_i$. To compare the population means $\mu_1$ and $\mu_2$ of the two conditions, we can test the null hypothesis $H_0: \mu_d = 0$. In fact $\mu_d = \mu_1 - \mu_2$:
$$
\mu_d = {1 \over m} \sum_{i=1}^m d_i = {1 \over m} \sum_{i=1}^m (y_{i1} − y_{i2}) =
{1 \over m} \sum_{i=1}^m y_{i1} - {1 \over m} \sum_{i=1}^m y_{i2} = \mu_1 - \mu_2
$$

#### c.

Now we want to do a significance test for the mean $\mu_d$.

We can assume that the differences $\{d_i = y_{i1} − y_{i2},\; i=1,...,8\}$ are approximately normally distributed since the mean response times $y_{i1}$ and $y_{i2}$ are approximately normally distributed for the Central Limit Theorem (since they are means). Actually, the significance test for the mean is quite robust and thus performs adequately even if this assumption is violated.

For this test, the test statistic is:
$$
T = {\bar Y - \mu_d \over {s \over \sqrt n}}
$$
with $\mu_d = 0$ because we assume that the null hypothesis is true.

This test statistic has a t-distribution with $df = n−1 = 7$ (instead of a standard normal distribution) because it uses the sample standard deviation $s$ (instead of the population standard deviation $\sigma$, which is unknown).

Let's explore the data file with the mean response times $y_{i1}$ and $y_{i2}$:

```{r}
data = read.table("http://stat4ds.rwth-aachen.de/data/CellPhone.dat", header=TRUE)
data
```

Let's calculate the observation of the test statistic $T$ with the formula above:

```{r}
diffs = data$phone - data$control #differences
T_obs = mean(diffs) * sqrt(8) / sd(diffs) #observation of T
T_obs
```

Finally, let's calculate the two-sided P-value:

```{r}
2 * pt(T_obs, df=7, lower.tail=FALSE) #2 times because two-sided
```

The P-value is the probability that the test statistic equals the observed value (or a more extreme one) assuming that $H_0$ is true. In this case it is quite low, meaning that we can confidently reject $H_0$ in favor of the alternative hypothesis $H_a: \mu_d \ne 0$. This indicates that cell phone use impairs drivers' reaction times.

### Ex 5.68

*Here the text of the second exercise.*

**Solution**

Add comments to the solution.


