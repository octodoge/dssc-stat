---
title: "GroupE_HW3"
author: "S. MADON KENGNE, M. Polo, S. Cappiello, R. Morsi"
date: "2022-12-2"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 4 

### Ex 4.24

*Refer to the vegetarian survey result in Exercise 4.6, with * $n$ *= 25 and no vegetarians.*\
$(a)$ *Find the Bayesian estimate of* $\pi$ *using a beta prior distribution with* $\alpha = \beta$ *equal:* $(i)$ *0.5,* $(ii)$ *1.0,* $(iii)$ *10.0.*\
*Explain how the choice of prior distribution affects the posterior mean estimate.* \
$(b)$ *If you were planning how to take a larger survey from the same population, explain how you can use the posterior results of the previous survey with* $n$ *= 25 based on the prior with* $\alpha = \beta = 1$ *to form the prior distribution to use with the new survey results.*

**Solution**

(a) The Bayesian estimate of pi is the mean of this posterior distribution. 
Since the prior distribution follow the Beta distribution and the likelihood follow the binomial distribution $(n,\pi)$, we can demonstrate that the posterior distribution follow a Beta distribution $Beta(\alpha*=\alpha+y,\beta*=\beta+n-y)$. So to get the Bayesian estimate we can just apply the formula $E(\pi|y)=\frac{\alpha*}{\alpha*+\beta*}$  
```{r}
n<-25
y<-0
alpha<-c(0.5,1.0,10.0)
beta<-alpha
alpha_star<-alpha+y
beta_star<-beta+n-y
bay_est<-alpha_star/(alpha_star+beta_star)
bay_est
```
We see that the posterior mean estimate increase with the parameters of the prior distribution.  The larger the values of $\alpha$ and $\beta$, the more weight the prior will have on the posterior estimate of $\pi$.


(b)  If we were planning to take a larger survey from the same population, we could use the posterior results of the previous survey with $n = 25$ and a beta prior with $\alpha = \beta = 1$ as the prior distribution for the new survey. This would involve using the updated parameters $\alpha *$ and $\beta *$ from the previous posterior distribution as the parameters for the beta prior in the new survey. This allows you to incorporate the information from the previous survey into the prior distribution for the new survey, which can help improve the accuracy of the posterior estimate of $\pi$.

### Ex 4.26

*Refer to the clinical trial example in Section 4.7.5. Using the Jeffreys prior for* $\pi_1$ *and for* $\pi_2$*, simulate and plot the posterior distribution of* $\pi_1 -\pi_2$*. Find the HPD interval. What does that interval reflect about the posterior distribution?*

**Solution**

something

### Ex 4.62

*For the bootstrap method, explain the similarity and difference between the true sampling distribution of* $\hat{\theta}$ *and the empirically-generated bootstrap distribution in terms of its center and its spread.*

**Solution**

something

## FSDS - Chapter 6

### Ex 6.10 

*The Students data file shows responses on variables summarized in Exercise 1.2.* \
$(a)$ *Fit the linear model using* `hsgpa` *= high school GPA,* `tv` *= weekly hours watching TV, and* `sport` *= weekly hours participating in sports as predictors of* `cogpa` *= college GPA. Report the prediction equation. What do the* $P$*-values suggest?* \
$(b)$ *Summarize the estimated effect of* `hsgpa`*.*\
$(c)$ *Report and interpret* $R^2$*, adjusted* $R^2$*, and the multiple correlation.*

**Solution**

something

### Ex 6.30

*When the values of* $y$ *are multiplied by a constant* $c$*, from their formulas, show that* $s_y$ *and* $\hat{\beta_1}$ *in the bivariate linear model are also then multiplied by* $c$*. Thus, show that* $r = \hat{\beta_1}(s_x/s_y)$ *does not depend on the units of measurement.*

**Solution**

Let's $Y_i=cy_i$.

We know that $\hat{\beta_1}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$ and $s_y=\frac{\sqrt{(y_i-\bar{y})^2}}{n-1}$. By replacing $y_i$ by $Y_i$, we have :

$new\hat{\beta_1}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(cy_i-c\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=\frac{c\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=c\hat{\beta_1}$ and

$news_y=\frac{\sqrt{(cy_i-c\bar{y})^2}}{n-1}=\frac{c\sqrt{(y_i-\bar{y})^2}}{n-1}=cs_y$.

So the new correlation is $newr = new\hat{\beta_1}\frac{s_x}{news_y}=c\hat{\beta_1}\frac{s_x}{cs_y}=\hat{\beta_1}\frac{s_x}{s_y}=r$.
Then, the correlation does not depend on the units of measurement.

### Ex 6.42

*You can fit the quadratic equation* $E(Y) = \beta_0+\beta_1 \cdot x+\beta_2 \cdot x^2$ *by fitting a multiple regression model with* $x_1 = x$ *and* $x_2 = x^2$*.* \
$(a)$ *Simulate* $100$ *independent observations from the model* $Y = 40.0-5.0 \cdot x+0.5\cdot x^2+\epsilon$*, where* $X$ *has a uniform distribution over* $\left[0, 10\right]$ *and* $\epsilon \sim \mathcal{N}(0,1)$*. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship.* \
$(b)$ *Find the correlation between* $x$ *and* $y$ *and explain why it is so weak even though the plot shows a strong relationship with a large* $R^2$ *value for the quadratic model.*

**Solution**

something

### Ex 6.52

$F$ *statistics have alternate expressions in terms of* $R^2$ *values.* \

$(a)$ *Show that for testing* $H_0: \beta_1 = \dots = \beta_p = 0$*,* 
$$
F = \frac{(TSS-SSE)/p}{SSE/[n-(p+1)]}
$$ 
*is equivalently*
$$
\frac{R^2/p}{(1-R^2)/[n-(p+1)]}
$$
*Explain why larger values of* $R^2$ *yield larger values of* $F$*.* \

$(b)$ *Show that for comparing nested linear models,*
$$
F = \frac{(SSE_0 - SSE_1)/(p_1 - p_0)}{SSE_1/[n-(p_1+1)]} = \frac{(R_1^2 - R_0^2)/(p_1-p_0)}{(1-R_1^2)/[n- (p_1+1)]}
$$

**Solution**

#### (a)

something

#### (b)

something

## LAB

*Suppose you receive* $n=15$ *phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is* $y_i \sim Exponential(\lambda)$*. Now, you have to choose the prior* $\pi(\lambda)$*. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:* \
- $\pi(\lambda)=\mathcal{B}eta(4,2)$*;*\
- $\pi(\lambda)=\mathcal{N}(1,2)$*;*\
- $\pi(\lambda)=\mathcal{G}amma(4,2)$*.*\
*Now, compute your posterior as* $\pi(\lambda|y) \propto L(\lambda;y)\cdot \pi(\lambda)$ *for the selected prior. If your first choice was correct, you will be able to compute it analytically.*

**Solution**

something 
