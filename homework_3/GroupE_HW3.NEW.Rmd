---
title: "GroupE_HW3"
author: "S. MADON KENGNE, M. Polo, S. Cappiello, R. Morsi"
date: "2022-12-2"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

```

## FSDS - Chapter 4 

### Ex 4.24

*Refer to the vegetarian survey result in Exercise 4.6, with * $n$ *= 25 and no vegetarians.*\
$(a)$ *Find the Bayesian estimate of* $\pi$ *using a beta prior distribution with* $\alpha = \beta$ *equal:* $(i)$ *0.5,* $(ii)$ *1.0,* $(iii)$ *10.0.*\
*Explain how the choice of prior distribution affects the posterior mean estimate.* \
$(b)$ *If you were planning how to take a larger survey from the same population, explain how you can use the posterior results of the previous survey with* $n$ *= 25 based on the prior with* $\alpha = \beta = 1$ *to form the prior distribution to use with the new survey results.*

**Solution**

(a) The Bayesian estimate of $\pi$ is the mean of its posterior distribution. 
Since the prior distribution follows a Beta distribution and the likelihood follows the binomial distribution $B(n,\pi)$, we can demonstrate that the posterior distribution is a Beta distribution $Beta(\alpha^*=\alpha+y,\beta^*=\beta+n-y)$. So to get the Bayesian estimate we can just apply the formula $E(\pi|y)=\frac{\alpha^*}{\alpha^*+\beta^*}$  
```{r}

n<-25
y<-0
alpha<-c(0.5,1.0,10.0)
beta<-alpha
alpha_star<-alpha+y
beta_star<-beta+n-y
bay_est<-alpha_star/(alpha_star+beta_star)
bay_est
```
We see that the posterior mean estimate increases with the parameters of the prior distribution.  The larger the values of $\alpha$ and $\beta$, the more weight the prior will have on the posterior estimate of $\pi$.


*(b)* If we were planning to take a larger survey from the same population, we could use the posterior results of the previous survey with $n = 25$ and a beta prior with $\alpha = \beta = 1$ to form the prior distribution for the new survey. This would involve using the updated parameters $\alpha^*$ and $\beta^*$ from the previous posterior distribution as the parameters for the beta prior in the new survey. This allows you to incorporate the information from the previous survey into the prior distribution for the new survey, which can help improve the accuracy of the posterior estimate of $\pi$.


### Ex 4.26

*Refer to the clinical trial example in Section 4.7.5. Using the Jeffreys prior for* $\pi_1$ *and for* $\pi_2$*, simulate and plot the posterior distribution of* $\pi_1 -\pi_2$*. Find the HPD interval. What does that interval reflect about the posterior distribution?*

**Solution**

In this example we have two binomial experiments, one with all success and the other is with a fail. i.e. $$Y_1 \sim Bin(11, \pi_1) \quad y_1 =11, \qquad Y_2 \sim Bin(1, \pi_2) \quad y_2 = 0 $$

The Jeffreys Prior for $\pi$ is $$p(\pi) \sim \sqrt{(I(\pi)}$$ where $I(\pi)$ is the Fischer information matrix. $$I(\pi) = - E\left[\frac{{\partial}^2 log(f(y|\pi))}{{\partial \pi}^2} | \pi \right]$$
Substituting with $f(y|\pi) = \pi^y (1 - \pi)^{n-y}$, we get that the Jeffreys prior is $\frac{1}{\sqrt{\pi (1-\pi)}}$ which is the beta distribution with hyperparameters $\alpha = \beta = \frac{1}{2}$, $Beta(1/2, 1/2)$.\ Hence, the posterior distributions of $\pi_1$ and $\pi_2$ are $Beta(11.5, 0.5)$, and $Beta(0.5, 1.5)$ respectively.
```{r}
library(HDInterval)
pi1 <- rbeta(1000000, 11.5, 0.5)
pi2 <- rbeta(1000000, 0.5, 1.5)
hdi1 <- hdi(pi1-pi2)
hdi1
hist(pi1-pi2)
plot(density(pi1-pi2))
pi1 <- rbeta(1000000, 110.5, 0.5)
pi2 <- rbeta(1000000, 0.5, 10.5)
hdi(pi1-pi2)

```
The posterior probability is $0.95$ that the probability of success with this treatment is between $(0.18, 1)$ higher than without it. The interval is wide due to the small size samples. The HDI of the sample experiment with larger sample sizes $n_1 =110, n_2 =10$ is $(0.82, 1)$.

### Ex 4.62

*For the bootstrap method, explain the similarity and difference between the true sampling distribution of* $\hat{\theta}$ *and the empirically-generated bootstrap distribution in terms of its center and its spread.*

**Solution**

Let $\hat \theta$ be the point estimate of a parameter $\theta$ calculated on a sample $y$. <br>
Let $y^{(1)},...,y^{(B)}$ be the samples obtained applying the bootstrap method on $y$. <br>
Let $\hat \theta^{(i)}$ be the point estimate of $\theta$ calculated on $y^{(i)}$.

The bootstrap distribution is the empirical distribution of $\hat \theta^{(1)},...,\hat \theta^{(B)}$. It is similar to the sampling distribution of $\hat \theta$ in terms of spread because they have similar shape and variability. However, they differ in terms of center because the bootstrap distribution is centered in $\hat \theta$ instead of $\theta$.

## FSDS - Chapter 6

### Ex 6.10 

*The Students data file shows responses on variables summarized in Exercise 1.2.* \
$(a)$ *Fit the linear model using* `hsgpa` *= high school GPA,* `tv` *= weekly hours watching TV, and* `sport` *= weekly hours participating in sports as predictors of* `cogpa` *= college GPA. Report the prediction equation. What do the* $P$*-values suggest?* \
$(b)$ *Summarize the estimated effect of* `hsgpa`*.*\
$(c)$ *Report and interpret* $R^2$*, adjusted* $R^2$*, and the multiple correlation.*

**Solution**

$(a)$  We first fit the model using hspga, sport and tv as explanatory variables for cogpa:

```{r}
datafile <-read.table("http://stat4ds.rwth-aachen.de/data/Students.dat", header=TRUE)
fit.students<-lm(datafile$cogpa~ datafile$hsgpa + datafile$tv + datafile$sport,data=datafile )
summary(fit.students)
```

Let's identify $y_i=i_{th}\ student's\ observed\ cogpa\ $ and $x_{i1},\ x_{i2},\ x_{i1}$ as hspga, tv, sport of the $i_{th}$ student respectively. We get the following equation:
$$\begin{aligned}
y_i &=\hat\beta_0 + \hat{\beta_1} x_{i1} + \hat{\beta_2} x_{i2} + \hat{\beta_3} x_{i3} \ +\epsilon_i\\
&=2.815+(0.2088) x_{i1}+(0.003336)x_{i2} -(0.014066)x_{i3}+\epsilon_i\\
&\approx2.815+(0.2088) x_{i1}+\epsilon_i
\end{aligned}$$

The predictive equation is then:
$$\begin{aligned}
y \approx2.815+(0.2088) x_{1}
\end{aligned}$$

The high p-values of $t_2=\frac{\hat\beta_2}{standarderror(\hat\beta_2)}$ and $t_3=\frac{\hat\beta_3}{standarderror(\hat\beta_3)}$ suggest us not to reject the null hypotheses $\beta_2=0\ and\ \beta_3=0$. The low p-values of $t_0=\frac{\hat\beta_0}{standarderror(\hat\beta_0)}$ and $t_1=\frac{\hat\beta_1}{standarderror(\hat\beta_1)}$ suggest us to reject the null hypotheses, so we assume $\beta_0\neq0\ and\ \beta_1\neq0$

$(b)$ Our model estimates that for each 1 point increase of the high school gpa  we would observe an increase of roughly 0.2 points in the college gpa of a student ($\hat\beta_1=0.21285$).

```{r}
fit.students1<-lm(datafile$cogpa~ datafile$hsgpa, data=datafile )
summary(fit.students1)
```

$(c)$ $R^{2}=0.1045$ and $R^{2}_{adjusted}=0.05655$.
The model can explain about the 10% of the variability of y=cogpa at best.
The multiple correlation is:
```{r}
cor(datafile$cogpa, fitted(fit.students))
```
The coefficient of multiple correlation takes values between 0 and 1. Higher values indicate higher predictability of the dependent variable from the independent variables, with a value of 1 indicating that the predictions are exactly correct and a value of 0 indicating that no linear combination of the independent variables is a better predictor than the fixed mean of the dependent variable $\bar{y}$.
The multiple correlation is also defined as the square root of the coefficient of determination ($\sqrt{R^2}$), as we can see we get the same result:
```{r}
sqrt(0.1045)
```

### Ex 6.30

*When the values of* $y$ *are multiplied by a constant* $c$*, from their formulas, show that* $s_y$ *and* $\hat{\beta_1}$ *in the bivariate linear model are also then multiplied by* $c$*. Thus, show that* $r = \hat{\beta_1}(s_x/s_y)$ *does not depend on the units of measurement.*

**Solution**

We know that $\hat{\beta_1}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$ and $s_y=\sqrt\frac{{\sum_{i=1}^{n}(y_i-\bar{y})^2}}{n-1}$. By replacing $y_i$ with $cy_i$, we have :

$\hat{\beta_1}'=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(cy_i-c\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=\frac{c\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=c\hat{\beta_1}$ and

$s_y'=\sqrt\frac{{\sum_{i=1}^{n}(cy_i-c\bar{y})^2}}{n-1}=c\sqrt\frac{{(\sum_{i=1}^{n}y_i-\bar{y})^2}}{n-1}=cs_y$.

So the new correlation is $r' = \hat{\beta_1}'\frac{s_x}{s_y'}=c\hat{\beta_1}\frac{s_x}{cs_y}=\hat{\beta_1}\frac{s_x}{s_y}=r$.
Therefore, the correlation does not depend on the units of measurement.

### Ex 6.42

*You can fit the quadratic equation* $E(Y) = \beta_0+\beta_1 \cdot x+\beta_2 \cdot x^2$ *by fitting a multiple regression model with* $x_1 = x$ *and* $x_2 = x^2$*.* \
$(a)$ *Simulate* $100$ *independent observations from the model* $Y = 40.0-5.0 \cdot x+0.5\cdot x^2+\epsilon$*, where* $X$ *has a uniform distribution over* $\left[0, 10\right]$ *and* $\epsilon \sim \mathcal{N}(0,1)$*. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship.* \
$(b)$ *Find the correlation between* $x$ *and* $y$ *and explain why it is so weak even though the plot shows a strong relationship with a large* $R^2$ *value for the quadratic model.*

**Solution**

(a)
```{r}
set.seed(123)
x <- sort(runif(101, 0,10))
x2 <- x*x
e = rnorm(101, 0,1)
y= 40 - 5*x + 0.5*x2 + e
fityx <- lm(y ~ x +x2)
summary(fityx)
coef(fityx)
plot(seq(0,10, 1/10), y, replace = FALSE)
cor(y,x)
cor(y,x2)
```

(b) The correlation between $y$ and $x$ is `r cor(y,x)` (very close to $0$), which show the lack of __linear__ relationship between $y$ and $x$. Indeed the scatter plot above reveals a nonlinear relationship between $x$ and $y$.

### Ex 6.52

$F$ *statistics have alternate expressions in terms of* $R^2$ *values.* \

$(a)$ *Show that for testing* $H_0: \beta_1 = \dots = \beta_p = 0$*,* 
$$
F = \frac{(TSS-SSE)/p}{SSE/[n-(p+1)]}
$$ 
*is equivalently*
$$
\frac{R^2/p}{(1-R^2)/[n-(p+1)]}
$$
*Explain why larger values of* $R^2$ *yield larger values of* $F$*.* \

$(b)$ *Show that for comparing nested linear models,*
$$
F = \frac{(SSE_0 - SSE_1)/(p_1 - p_0)}{SSE_1/[n-(p_1+1)]} = \frac{(R_1^2 - R_0^2)/(p_1-p_0)}{(1-R_1^2)/[n- (p_1+1)]}
$$

**Solution**

#### (a)

Let $TSS$ be the Total Sum of Squares and $SSE$ be the Sum of Squared Errors (also known as residual sum of squares). The coefficient of determination $R^2$ is defined as:
$$
R^2 = {TSS - SSE \over TSS} = 1 - {SSE \over TSS}
$$
Therefore:
$$
{R^2/p \over (1-R^2)/[n-(p+1)]} =
{{TSS - SSE \over TSS}/p \over {SSE \over TSS}/[n-(p+1)]} =
{(TSS - SSE)/p \over SSE/[n-(p+1)]} = F
$$
Larger values of $R^2$ yield larger values of $F$ because $0 \le R^2 \le 1$.

#### (b)

Let the model $M_0$ be nested in the model $M_1$ (meaning they have the same response variable $Y$ but the explanatory variables of $M_0$ are a subset of those of $M_1$). <br>
The Total Sum of Squares $\; TSS = \sum_i (y_i - \bar y)^2 \;$ is the same for both models (because it only depends on the observations $y_i$ of $Y$ and their mean $\bar y$). <br>
In this case, the coefficient of determination of $M_i$ is defined as:
$$
R_i^2 = 1 - {SSE_i \over TSS}
$$
Therefore:
$$
\frac{(R_1^2 - R_0^2)/(p_1-p_0)}{(1-R_1^2)/[n- (p_1+1)]} =
{\left( 1-{SSE_1 \over TSS} - 1+{SSE_0 \over TSS} \right)/(p_1-p_0) \over {SSE_1 \over TSS}/[n-(p_1+1)]} =
\frac{(SSE_0 - SSE_1)/(p_1 - p_0)}{SSE_1/[n-(p_1+1)]} = F
$$

## LAB

*Suppose you receive* $n=15$ *phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is* $y_i \sim Exponential(\lambda)$*. Now, you have to choose the prior* $\pi(\lambda)$*. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:* \
- $\pi(\lambda)=\mathcal{B}eta(4,2)$*;*\
- $\pi(\lambda)=\mathcal{N}(1,2)$*;*\
- $\pi(\lambda)=\mathcal{G}amma(4,2)$*.*\
*Now, compute your posterior as* $\pi(\lambda|y) \propto L(\lambda;y)\cdot \pi(\lambda)$ *for the selected prior. If your first choice was correct, you will be able to compute it analytically.*

**Solution**

Let's visualize the three possible prior distributions $\pi(\lambda)$:

```{r}
par(mfrow = c(2, 2))
curve(dbeta(x, 4, 2), xlim=c(0,1), ylim=c(0, 2.1), ylab=expression(Beta(lambda)), xlab=expression(lambda))
curve(dnorm(x, mean=1, sd=sqrt(2)),  xlim=c(-3, 5), ylim=c(0, 0.3), ylab=expression(N(lambda)), xlab=expression(lambda))

curve(dgamma(x,shape=4,rate=2), xlim=c(0,5), ylim=c(0, 0.5), ylab=expression(Gamma(lambda)), xlab=expression(lambda))


```


The first one, $\pi(\lambda)=Beta(4,2)$, is not adequate because we would only model values $\lambda\in[0,1]$ ($\Rightarrow E(Y)=\frac{1}{\lambda} \in[1,+\infty)$, i.e. we would only model phone calls of length with mean greater than 1).
The second one, $\pi(\lambda)=N(1,2)$, is not suitable for the phenomenon we are taking into account because it models negative values of $\lambda$, which are not of our interest.
$\pi(\lambda)=Gamma(4,2)$ as prior distribution is instead a plausible distribution for $\lambda$ (phone calls that roughly last, on average, between 0.20 seconds and 2 minutes and less likely to occur as they depart from this interval).

We now compute the posterior distribution $\pi(\lambda|y)	\propto  L(\lambda;y)\pi(\lambda)$, knowing that $L(\lambda;y)=\lambda^{n} e^{-\lambda \sum_{i=1}^{n}y_i}=\lambda^{15} e^{-\lambda \sum_{i=1}^{15}y_i}$ and $\pi(\lambda)=Gamma(4,2)=\frac{2^4}{6}\lambda^{3}e^{-2\lambda}$:

$$
\begin{aligned}
\pi(\lambda|y) 	&\propto \lambda^{n} e^{-\lambda \sum_{i=1}^{n}y_i} \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda} \\
&\approx \lambda^{15} e^{-\lambda \sum_{i=1}^{15}y_i} \lambda^{3}e^{-2\lambda}\\
&\approx\lambda^{18}e^{-\lambda((\sum_{i=1}^{15}y_i) +2)}
\end{aligned}
$$


The posterior is a Gamma distribution with $\alpha=19$ and $\beta=\sum_{i=1}^{15}y_i + 2$.
