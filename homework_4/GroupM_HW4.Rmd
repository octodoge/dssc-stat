---
title: "GroupM_HW4"
author: "S. Lippolis, F. Spreafichi, M. Polo"
date: "2022-12-30"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 7

### Ex 7.4

*Analogously to the previous exercise, randomly sample 30 X observations from a uniform in
the interval (-4,4) and conditional on X = x, 30 normal observations with E(Y ) = 3.5x3 −
20x2 + 0.5x + 20 and σ = 30. Fit polynomial normal GLMs of lower and higher order than
that of the true relationship. Which model would you suggest? Repeat the same task for
E(Y ) = 0.5x3 − 20x2 + 0.5x + 20 (same σ) several times. What do you observe? Which model
would you suggest now?*

**Solution**

First data:
```{r, include=TRUE}
### 1.

## Build the data

# Randomly sample x observations with uniform distribution in (-4,4)
x<-runif(30,min=-4,max=4)

# Mean of each resposne variable
y_mean<-3.5*x^3 - 20*x^2 + 0.5*x + 20

# The response variable follows a normal distribution of mean y_mean
#   and standard deviation sd
y<-rnorm(30,mean=y_mean,sd=30)



## Build models

# Build dataframe
df<-data.frame(y,x)

# Build GLMs
fit0<-glm(formula = y~1, data=df , family=gaussian)
fit1<-glm(formula = y~x, data=df,family=gaussian)
fit2<-glm(formula = y~x+I(x^2), data=df,family=gaussian)
fit3<-glm(formula = y~x+I(x^2)+I(x^3), data=df,family=gaussian)

## Assess models

# Save summury of the models
model0<-summary(fit0)
model1<-summary(fit1)
model2<-summary(fit2)
model3<-summary(fit3)

# Extract AIC of each model
rbind(model0$aic, model1$aic,model2$aic,model3$aic)

```
I would suggest the last model. \
Because the lower is the AIC, the better is the model.



Second data:

```{r, include=TRUE}
### 2.

## Build the data

# Randomly sample x observations with uniform distribution in (-4,4)
set.seed(24)
x<-runif(30,min=-4,max=4)

# Mean of each resposne variable
y_mean<-0.5*x^3 - 20*x^2 + 0.5*x + 20

# The response variable follows a normal distribution of mean y_mean
#   and standard deviation sd
y<-rnorm(30,mean=y_mean,sd=30)



## Build models

# Build dataframe
df<-data.frame(y,x)

# Build GLMs
fit0<-glm(formula = y~1, data=df , family=gaussian)
fit1<-glm(formula = y~x, data=df,family=gaussian)
fit2<-glm(formula = y~x+I(x^2), data=df,family=gaussian)
fit3<-glm(formula = y~x+I(x^2)+I(x^3), data=df,family=gaussian)

## Assess models

# Save summury of the models
model0<-summary(fit0)
model1<-summary(fit1)
model2<-summary(fit2)
model3<-summary(fit3)

# Extract AIC of each model
rbind(model0$aic, model1$aic,model2$aic,model3$aic)

```
Now AIC says that the better model is the 2nd. \
I think that the impact of the polynomial term of order 3 has no a big impact. Then AIC suggests the second model that is a good compromise between fit on data and complexity.

### Ex 7.20

*In the* `Crabs` *data file introduced in Section 7.4.2, the variable y indicates whether a female horseshoe crab has at least one satellite (1 = yes, 0 = no).*

a. *Fit a main-effects logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a 95% confidence interval for the weight effect.*

b. *Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.*

c. *Use AIC to determine which models seem most sensible among the models with (i) interaction, (ii) main effects, (iii) weight as the sole predictor, (iv) color as the sole predictor, and (v) the null model.*

**Solution**

#### a.

Let's explore the `Crabs` data file:

```{r}
data = read.table("https://stat4ds.rwth-aachen.de/data/Crabs.dat", header=TRUE)
head(data, n=3) #show first 3 rows
```
\
Let's fit the model:

```{r}
model = glm(
  formula = y ~ weight + factor(color), #factor to make color categorical
  family="binomial", #binomial because y is binary
  data=data)
coef(summary(model))
```
\
The summary above shows the p-values (`Pr(>|z|)` column) of the Wald tests for the effect of each variable. But each color level is tested separately (1st one missing because redundant). So it's better to conduct a likelihood-ratio test in this case:

```{r message=FALSE}
library(car)
Anova(model) #likelihood-ratio test by default
```

A p-value of $0.06594$ suggests that the color may have effect on $y$ but there is no strong evidence.\
\
Finally, let's construct the confidence interval for the weight effect:

```{r message=FALSE}
confint(model, parm='weight') #95% confidence by default
```

Note that these values don't represent the bounds for the effect on $y$ directly, but the effect on log odds (because of the logit link function of the binomial family).\
\

#### b.

Let's fit the interaction model:

```{r}
model = glm( #same as before but with interaction term
  formula = y ~ weight + factor(color) + weight:factor(color),
  family="binomial",
  data=data)
coef(summary(model))
```

The summary above shows the estimated effect of weight on log odds for each color (`Estimate` column, last 3 rows).\
\
Finally, let's test whether this model provides a better fit:

```{r}
Anova(model) #likelihood-ratio test by default
```

A p-value of $0.07562$ suggests that the interaction term may provide a better fit but there is no strong evidence.\
\

#### c.

Let's compare the 5 models:

```{r}
#define model formulas
formulas = list(
  interaction = y ~ weight + factor(color) + weight:factor(color),
  main_effects = y ~ weight + factor(color),
  weight_only = y ~ weight,
  color_only = y ~ factor(color),
  null_model = y ~ 1)

#fit models
models = lapply(formulas, function(formula) glm(
  formula=formula,
  family='binomial',
  data=data))

#compare models
lapply(models, AIC)
```

The lower the AIC score, the better, so the interaction model seems the most sensible.

### Ex 7.26

*Here the text of the exercise.*

**Solution**

Add comments to the solution.

### Ex 7.28

*For the Students data file, model y = whether you are a vegetarian with the 14 binary and
quantitative explanatory variables.
(a) For ordinary logistic regression, show that this model fits better than the null model, but
no Wald tests are significant.
(b) Use the lasso, with λ to minimize the sample mean prediction error in cross-validation.
Compare estimates to those from ordinary logistic regression.
(c) Specify the range of λ values for which the lasso smoothing generates the null model.*

**Solution**

* Point (a):

```{r, echo=T}
## Data

# Import data set
students=read.table("http://stat4ds.rwth-aachen.de/data/Students.dat",header=TRUE)



## Models

# Build the logit null model
fit0<-glm(formula = veg~1, data = students,
            family = binomial(link="logit"))

# Build the logit model (with all the possible explanatory variables)
fit14<-glm(formula = veg~., data = students,
                family = binomial(link="logit"))


## Assess models: compare the error

# Error of the null model
pred0 <- predict(fit0, type="response")

err.rate0 <- mean((pred0 > 0.5 & students$veg ==0) |
                     ((pred0 < 0.5 & students$veg ==1 )))
err.rate0

# Error of the complete model
pred14 <- predict(fit14, type="response")

err.rate14 <- mean((pred14 > 0.5 & students$veg ==0) |
                    ((pred14 < 0.5 & students$veg ==1 )))
err.rate14
```

The complete model fits better then the null model because has less error.

```{r, echo=TRUE}
# Build the logit model (with all the possible explanatory variables)
fit14<-glm(formula = veg~., data = students,
                family = binomial(link="logit"))

# Significance of variable
summary(fit14)
```
There aren't significant variables.

* Point (b):


```{r, echo=T}
## Organize data
attach(students)
x <- cbind(gender, abor, age, hsgpa, cogpa, dhome, dres, tv, sport, news,
              aids, ideol, relig, affirm) # explanatory variables for lasso

## Fit the lasso model
library(glmnet)
fit.lasso <- glmnet(x, veg, alpha=1, family="binomial"(link = "logit")) # alpha=1 selects lasso

# Lambda graph
plot(fit.lasso, "lambda")

# Cross validation to find the better lambda
set.seed(1) # a random seed to implement cross-validation
cv <- cv.glmnet(x, veg, alpha=1, family="binomial"(link = "logit"))
cv$lambda.min # best lambda by 10-fold cross-validation
cv$lambda.1se # lambda suggested by one-standard-error rule, a random variable

# Compare the coefficient of the model with the minimum lambda
coef(glmnet(x, veg, alpha=1, family="binomial"(link = "logit"), lambda=0.0783021))
coef(glmnet(x, veg, alpha=1, family="binomial"(link = "logit"), lambda=0.09431516))
```
The model with the minimum lambda for the cross validation process reveals that
there is a dependency between veg (be vegetarian) and affirm (support affirmative causes).

* Point (c):

```{r, echo=T}
# Find the model different form the null model but with the maximum lambda
coef(glmnet(x, veg, alpha=1, family="binomial"(link = "logit"), lambda=0.0943))
coef(glmnet(x, veg, alpha=1, family="binomial"(link = "logit"), lambda=0.0944))
```
The null model is obtained for $\lambda >\approx 0.0943$.

### Ex 7.42

*Here the text of the exercise.*

**Solution**

Add comments to the solution.

### Ex 7.44

*For a sequence of independent binary trials, Exercise 2.69 showed the probability distribution of* $Y=$ *the number of successes before the* $k$*th failure. Show that this distribution is a special case of the negative binomial distribution (7.7), for* $\pi = \mu/(\mu + k)$*. (The geometric distribution is the special case* $k=1$*.)*

**Solution**

The probability mass function of $Y$ is:
$$
f(y;k,\pi) = {y+k-1 \choose y} \pi^y (1-\pi)^k, \quad y=0,1,2,...
$$
$\quad$ with the binomial coefficient defined as ${n \choose y} = {n! \over (n-y)!y!}$.\
\
The probability mass function of the negative binomial distribution is:
$$
p(y;\mu,k) = {\Gamma(y+k) \over \Gamma(k)\Gamma(y+1)}
             \left(\mu \over \mu+k\right)^y
             \left(k \over \mu+k\right)^k, \quad y=0,1,2,...
$$
$\quad$ with the gamma function defined as $\Gamma(n)=(n-1)!$.\
\
Let $\pi = {\mu \over \mu + k} = 1-{k \over \mu+k}$:
$$
p(y;\mu,k) = {(y+k-1)! \over (k-1)!y!} \pi^y (1-\pi)^k = f(y;k,\pi), \quad y=0,1,2,...
$$

So the distribution of $Y$ is a special case of the negative binomial distribution.

## FSDS - Chapter 8 

### Ex 8.4

*Refer to Exercise 8.1. Construct a classification tree, and prune strongly until the tree uses a single explanatory variable. Which crabs were predicted to have satellites? How does the proportion of correct predictions compare with the more complex tree in Figure 8.2?*

**Solution**

Exercise 8.1 is about the `Crabs` data file. Let's explore it:

```{r}
data = read.table("https://stat4ds.rwth-aachen.de/data/Crabs.dat", header=TRUE)
head(data, n=3) #show first 3 rows
```
\
Let's construct the tree:

```{r}
library(rpart)
library(rpart.plot)
tree = rpart(
  formula = y ~ weight + width + color + spine,
  method="class", #class because y is binary
  data=data)
rpart.plot(tree, extra=2) #extra=2 to show proportion of correct predictions
```
\
Let's print the complexity parameter table to figure out where to prune the tree:

```{r}
printcp(tree, digits=2) #digits=2 to round values
```
\
The table above shows that, to get a tree with a number of splits `nsplit` $=1$ (and so a single explanatory variable), we must prune with a complexity parameter `CP` $\approx 0.081$. Let's prune it:

```{r}
pruned = prune(tree, cp=0.081)
rpart.plot(pruned, extra=2)
```
\
The crabs predicted to have satellites ($y=1$) are the ones with width $\ge 26$ cm.

The proportion of correct prediction is ${44+77 \over 173} = 0.699$, that is lower than the one of the more complex tree in Figure 8.2, which is reported to be $0.751$.
