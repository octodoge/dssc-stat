---
title: "FinalProjectGroupC"
author: "me"
date: "2023-01-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(corrplot)

path  = 'C:/Users/Win10/Desktop/winequality-white.csv'
d = read.csv(path, header = T, sep = ';')
str(d)
summary(d)
  #plot the correlation matrix
corrplot(cor(d), method = "shade")
```


```{r}
#We can plot a lot of histograms to better visualize the data
hist(d$fixed.acidity, breaks = 100)
hist(d$volatile.acidity, breaks = 100)
hist(d$citric.acid, breaks = 100)
hist(d$residual.sugar, breaks = 100)
hist(d$chlorides, breaks = 100)
hist(d$free.sulfur.dioxide, breaks = 100)
hist(d$total.sulfur.dioxide, breaks = 100)
hist(d$density, breaks = 100)
hist(d$pH, breaks = 100)
hist(d$sulphates, breaks = 100)
hist(d$alcohol, breaks = 100)
hist(d$quality)

```


From the plots we identify some possible outliers, removing them could be useful to improve the results
```{r}
d_clean = d
for (j in c(1, 2, 3, 4, 5, 6, 7, 8)){
  max = max(d_clean[,j])
  print(paste("The maximum value for the variable", j, "is", max))
  previous_dimension = nrow(d_clean)
  
  d_clean = d_clean[-(which(d_clean[,j]==max)),] #we remove the outliers
  
  new_max = max(d_clean[,j])
  following_dimension = nrow(d_clean)
  print(paste("We removed", previous_dimension-following_dimension, "lines, now the max is", new_max)) #Just to check how many lines we remove at the time
}

```

We also note that there aren't many observations with quality = 9 or quality = 3. This could lead to classifications problems (r will suggest to use LOOCV instead of the classical CV)

```{r}

d[(d$quality == 3) | (d$quality == 9),]


```


```{r}

fit1 = glm(quality ~ ., family = gaussian(link = "identity"), data = d)
summary(fit1)

par(mfrow=c(2,2))
plot(fit1)


```
Using the cleaned data we get more significant results
```{r}

fit2 = glm(quality ~ ., family = gaussian(link = "identity"), data = d_clean)
summary(fit2)

par(mfrow=c(2,2))
plot(fit2)

```

Since we see a drop in value of the AIC we assume we can remove those outliers.
We can also see in the leverage graph that we removed the point with greater leverage and that we do not expect to see any more outliers.


Now we try to analyze the importance of certain features with the lasso method. A necessary phase before using LASSO is to normalize the dataset so that every variable is on the same scale.

```{r}
#rescaling d_clean
d_clean.scale = as.data.frame(scale(d_clean[,1:11]))
quality = d_clean$quality
d_clean.scale = cbind(d_clean.scale, quality)
#print(mean(d_clean.scale$fixed.acidity))
#print(var(d_clean.scale$fixed.acidity))
rm(quality)
```


```{r warning=FALSE, message=FALSE}
attach(d_clean.scale)

x = cbind(fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol)

set.seed(4)  #we fix a seed for the random operations of cv

cv = cv.glmnet(x, quality, alpha=1, family=gaussian(link = "identity"), type.measure="mse", nfolds = 10) #we do a 10 fold CV with mean squared error loss function
```


```{r}
lambda.min = cv$lambda.min#we find the best value of lambda

fit.lasso = glmnet(x, quality, alpha=1, family=gaussian(link = "identity"), lambda = lambda.min)

coef(fit.lasso)

fit.lasso = glmnet(x, quality, alpha=1, family=gaussian(link = "identity"))
plot(fit.lasso)

```
From the LASSO method we get that the best performing model is the one that uses all the variables. $rivedi$


Now we try to use the GAM model
```{r}
library(mgcv)

fit_gam = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_clean, family = gaussian(link = "identity")) 

summary(fit_gam)

plot(fit_gam)

print(fit_gam$aic)

```

Now we consider the different values of the smoothing parameters in the GAM and how we could improve them. We do this by checking if for each feature it would be better to use the already found smoothing parameter or its double.
```{r}
sp1 = fit_gam$sp

sp2 = 2 * sp1
t = rbind(sp1,sp2) #creates a matrix with sp1 and sp2 as rows

sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10], t[,11]) # creates a dataframe whose rows are the different possible combinations of smoothing parameters
n.tuning = nrow(sp.grid)
#Now I create the vector of the different AIC for each combination of values
aic = rep(NA, n.tuning)
#And now I perform the grid search
for(i in 1:n.tuning){
 gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_clean, family = gaussian(link = "identity"), sp = sp.grid[i,]) 
 
 aic[i] = AIC(gamobj)
 
}
# Find the minimum
opt.sp = sp.grid[which.min(aic),]
print(opt.sp) 

```







