---
title: "FinalProjectGroupC"
author: "me"
date: "2023-01-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(corrplot)

path  = 'C:/Users/Win10/Desktop/winequality-white.csv'
d = read.csv(path, header = T, sep = ';')
str(d)
summary(d)
  #plot the correlation matrix
corrplot(cor(d), method = "shade")
```

From the correlation matrix we consider the possibility that the variable "density" has high colinearity with respect to the others. To prove that and to consider the possibility of removing the variable we look at the VIF coefficients:
```{r}
library(car)
model = lm(quality ~ ., data = d)
vif(model)
```
From this analysis we conclude that the variable "density" has high colinearity to the others and could create problems for the perfomrmances of linear model.


To better analyze the distributions of values for each feature we can plot the following histograms
```{r}
hist(d$fixed.acidity, breaks = 100)
hist(d$volatile.acidity, breaks = 100)
hist(d$citric.acid, breaks = 100)
hist(d$residual.sugar, breaks = 100)
hist(d$chlorides, breaks = 100)
hist(d$free.sulfur.dioxide, breaks = 100)
hist(d$total.sulfur.dioxide, breaks = 100)
hist(d$density, breaks = 100)
hist(d$pH, breaks = 100)
hist(d$sulphates, breaks = 100)
hist(d$alcohol, breaks = 100)
hist(d$quality)
```




From the plots we identify some possible outliers, removing them could be useful to improve the results
```{r}
#we obtain the sigmas and the mean of each column of features
sigmas = rep(NA,11)
means = rep(NA,11)
for (j in 1:11){
  sigmas[j] = sd(d[,j])
  means[j] = mean(d[,j])
}

#now we exclude the samples whose feature are outside the 4sigma intervals
lower = means - 3*sigmas
higher = means + 3*sigmas
print(cbind(lower,higher))
#we create the list "delete" that finds which rows are to delete in the dataframe
delete = rep(F, nrow(d))
#print(cbind(d[,1],higher[1], d[,1]>higher[1]))
for (j in 1:11){
  delete = (delete | d[,j]<lower[j] | d[,j]>higher[j])
}
print(paste("We removed:",sum(delete),"samples from our data"))  #how many observations are deleted
#now we delete the the suspected samples from the dataframe
d_clean = d[-(which(delete)),]
```
Now we plot the new histograms without the supposed outliers
```{r}
#We can plot a lot of histograms to better visualize the data
hist(d_clean$fixed.acidity, breaks = 100)
hist(d_clean$volatile.acidity, breaks = 100)
hist(d_clean$citric.acid, breaks = 100)
hist(d_clean$residual.sugar, breaks = 100)
hist(d_clean$chlorides, breaks = 100)
hist(d_clean$free.sulfur.dioxide, breaks = 100)
hist(d_clean$total.sulfur.dioxide, breaks = 100)
hist(d_clean$density, breaks = 100)
hist(d_clean$pH, breaks = 100)
hist(d_clean$sulphates, breaks = 100)
hist(d_clean$alcohol, breaks = 100)
hist(d_clean$quality)

```




##Linear models

Now analyze the data using the linear models, we start by analyzing the results obtained using the starting dataset.
```{r}

fit1 = glm(quality ~ ., family = gaussian(link = "identity"), data = d)
summary(fit1)

par(mfrow=c(2,2))
plot(fit1)


```
Using the cleaned data we get more significant results
```{r}

fit2 = glm(quality ~ ., family = gaussian(link = "identity"), data = d_clean)
summary(fit2)

par(mfrow=c(2,2))
plot(fit2)

```

Since we see a drop in value of the AIC we assume we can remove those outliers.
We can also see in the leverage graph that we removed the point with greater leverage and that we do not expect to see any more outliers.

###LASSO
Now we try to analyze the importance of certain features with the lasso method. A necessary phase before using LASSO is to normalize the dataset so that every variable is on the same scale.

```{r}
#rescaling d_clean
d_clean.scale = as.data.frame(scale(d_clean[,1:11]))
quality = d_clean$quality
d_clean.scale = cbind(d_clean.scale, quality)
#print(mean(d_clean.scale$fixed.acidity))
#print(var(d_clean.scale$fixed.acidity))
rm(quality)
```


```{r warning=FALSE, message=FALSE}
attach(d_clean.scale)

x = cbind(fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol)

set.seed(4)  #we fix a seed for the random operations of cv

cv = cv.glmnet(x, quality, alpha=1, family=gaussian(link = "identity"), type.measure="mse", nfolds = 10) #we do a 10 fold CV with mean squared error loss function
```


```{r}
lambda.min = cv$lambda.min#we find the best value of lambda

fit.lasso = glmnet(x, quality, alpha=1, family=gaussian(link = "identity"), lambda = lambda.min)

coef(fit.lasso)

fit.lasso = glmnet(x, quality, alpha=1, family=gaussian(link = "identity"))
plot(fit.lasso)

```
From the LASSO method we get that the best performing model is the one that uses all the variables. 

##GAM
Now we try to use the GAM model to analyze the possibility of non-linear relationships between the features.
```{r}
library(mgcv)

fit_gam = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_clean, family = gaussian(link = "identity")) 

summary(fit_gam)

plot(fit_gam)

print(fit_gam$aic)

```

Now we consider the different values of the smoothing parameters in the GAM and how we could improve them. We do this by checking if for each feature it would be better to use the already found smoothing parameter or its double.
```{r}
sp1 = fit_gam$sp

sp2 = 2 * sp1
t = rbind(sp1,sp2) #creates a matrix with sp1 and sp2 as rows

sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10], t[,11]) # creates a dataframe whose rows are the different possible combinations of smoothing parameters
n.tuning = nrow(sp.grid)
#Now I create the vector of the different AIC for each combination of values
aic = rep(NA, n.tuning)
#And now I perform the grid search
for(i in 1:n.tuning){
 gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_clean, family = gaussian(link = "identity"), sp = sp.grid[i,]) 
 
 aic[i] = AIC(gamobj)
 
}
# Find the minimum
opt.sp = sp.grid[which.min(aic),]
print(opt.sp) 

```







