- vedi GAM da lab 8/ ultime slide
  - spline puoi usarle anche su GLM ma la differenza è che GAM ha una penalizzazione nel fitting che GLM non ha quindi non è la stessa cosa (https://stats.stackexchange.com/a/298831 o lab 8): quando le specifichi su GLM (con `bs()`) specifichi anche il grado dei polinomi (e quindi numero di coefficienti) e fai linear regression sui coefficienti, invece su GAM le specifichi non-parametriche (con `s()`) e ci pensa il fitting a trovare le migliori (smoothin regression): https://stats.stackexchange.com/a/517784

- vedi dataset per capire quale family usare per GAM/GLM (gaussian, poisson, binomial, negative binomial)
  - y é multiclass ma con classi ordinate, però credo che nessuna delle famiglie che abbiamo visto vada bene. Serve famiglia che modelli ordered categorical data (diverso da count della poisson o negative binomial perché limitate). Per multiclass di solito si usa multinomial (che forse non abbiamo fatto). Ma visto che le nostre sono ordered non trascurerei questo fatto e farei ordinal logistic regression quindi con family ordinal (che non credo abbiamo fatto, e forse è da importare o usare altra API rispetto `glm`). Inoltre neanche l'API `glmnet` per lasso penso abbia ordinal, però ha multinomial
    - anzi imo vai easy con linear regression (family gaussian, o usa direttamente)
  - per alberi invece non credo esista ordinal regression. o fai regression o classification. Differenza è tipo che in regression se 2 alberi predicono 5 e 3 predicono 7, viene classificato come 6 (ma devi fare tu qualche trick per arrotondare), mentre in classification come 7.

- Per interpretabilità random forest (e boosting) vedi slide alberi, oppure anche slide machine learning di Medved, e vedi se per caso c'è già modo easy in R (in teoria sì, vedi i mean decrease alla fine del lab 8)

- forse serve vedere anche correlazione tra le variabili come in lab 8. Ma dopo che te ne fai? scarti le predictor correlate tra loro?
  - Correlated variables in a dataset can lead to multicollinearity, which can affect the interpretation of the coefficients. It's generally recommended to address correlated variables when training a GLM. One way to do this is by discarding one of the correlated variables. Another way is to use regularization techniques such as Lasso, which can help to reduce the impact of correlated variables on the model by shrinking the coefficients of correlated variables.

- Lasso dovrebbe essere generalizzazione di GLM quindi basta quello imo (con lambda=0 è GLM normale penso, ma non so se la  sua API `glmnet` permette di fare le stesse cose). Vedi [qui](https://stackoverflow.com/a/38378121) per confronto risultati con glm normale per vedere se si sta facendo bene. IMO usa cv.glmnet per trovare lambda migliore. Fa così anche libro.

- models (fai linear regression):
  - gaussian glm (or lm but less cool)
    - tutte variabili così come sono più tutte interaction primo grado e basta (niente quadrati)
  - gaussian gam
    - spline di tutte le variabili così come sono e basta
  - random forest
  - boosting (se c'è tempo, meglio no)

---

# Selection, description, and comparison of the most suitable statistical models

```{r}
#gam (esempio)
n<-200
sig <- 2
dat <- gamSim(1,n=n,scale=sig)

b<-gam(y~s(x0)+s(I(x1^2))+s(x2)+offset(x3),data=dat)

newd <- data.frame(x0=(0:30)/30,x1=(0:30)/30,x2=(0:30)/30,x3=(0:30)/30)
actual = rep(5, 31)
pred <- predict.gam(b,newd)
mse = mean((pred - actual)^2) #actual non definito in questo esempio
mse
```

## Random Forest

* Two way of analyzing data:
  * **Statistical models** (eg. GLM, GAM)
    * Focus on interpretation
    * Describe relationship between variables in functional form
  * **Algorithms** (eg. decision tree)
    * Focus on prediction

* Decision tree (**regression tree** for our case):
  * Built by recursively splitting the data
    * Each node of the tree corresponds to a split
    * Each split is based on values of a feature
      * The one that allows the best split is selected
    * Splitting continue until a stopping criteria is met
      * Eg. min number of data in a node or max tree depth
  * Prediction made by traversing the tree until a leaf node
    * Predicted value is the mean of values in that node

* **Random forest**:
  * 

MA IN TEORIA NO MEAN GINI DECREASE FOR REGRESSION: studia doc random forest
vedi anche [qui](https://stats.stackexchange.com/a/162590)


# Comments on results
