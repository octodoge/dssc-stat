## General Additive Model (GAM)

Now we try to use the GAM model to analyze the possibility of non-linear relationships between the features.

```{r}
library(mgcv)
#we fit the GAM model on the different datasets
#On d_train
gam_train = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_train, family = gaussian(link = "identity")) 

summary(gam_train)


#On d_train_clean
gam_clean = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_train_clean, family = gaussian(link = "identity")) 

summary(gam_clean)


#On d_train_light
gam_light = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(pH) + s(sulphates) + s(alcohol), data = d_train_light, family = gaussian(link = "identity")) 

summary(gam_light)


#On d_train_light_clean
gam_light_clean = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(pH) + s(sulphates) + s(alcohol), data = d_train_light_clean, family = gaussian(link = "identity")) 

summary(gam_light_clean)
```

Based on the analysis of the $R^2$ coefficient, we can see that the model trained on $D_{train}^{clean}$ is slightly better than the others. So we decide to visualize the function applied by this model to our features.
```{r}
plot(gam_clean)
```
These graphs help us discover the non linear effect that each variable has, something that was not explained in the others linear model.


Now we consider the different values of the smoothing parameters in the GAM and how we could improve them. We do this by checking if for each feature it would be better to use the already found smoothing parameter or its double. The evaluation for the smoothing parameters is based on the AIC of the relative model.


1. For $D_{train}$
```{r}
library(parallel)
library(iterators)
library(foreach)
library(doParallel)

sp1_train = gam_train$sp
sp2_train = 2 * sp1_train
t = rbind(sp1_train,sp2_train)

#we create the dataframe sp.grid whose rows are the different possible combinations of smoothing parameters
sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10], t[,11])
n.tuning = nrow(sp.grid)

#We create a list of the rows of sp.grid 
sp.list = split(sp.grid, seq(n.tuning))

#Now we create the function that for each vector of parameters returns the corresponding AIC of the model.
fun  = function(parameters){
  gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_train, family = gaussian(link = "identity"), sp = parameters)
  AIC(gamobj)
}
  
#And now we perform the grid search in parallel
split = detectCores() - 1  #detects the number of cores
cl = makeCluster(split) #creates the parallel environment
init = clusterEvalQ(cl, { library(mgcv); NULL }) #sets up the environment
clusterExport(cl, "d_train")  #exports the dataset to the other clusters

AICs = parLapplyLB(
  cl,   #The cluster element
  sp.list,    #The list where we apply the function
  fun  #The function applied to the list
)
# Find the minimum
opt.sp_train = sp.grid[which.min(AICs),]
print(opt.sp_train)
stopCluster(cl)
```

2. For $D_{train}^{light}$
```{r}
sp1_light = gam_light$sp
sp2_light = 2 * sp1_light
t = rbind(sp1_light,sp2_light)

#we create the dataframe sp.grid whose rows are the different possible combinations of smoothing parameters
sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10])
n.tuning = nrow(sp.grid)

#We create a list of the rows of sp.grid 
sp.list = split(sp.grid, seq(n.tuning))

#Now we create the function that for each vector of parameters returns the corresponding AIC of the model.
fun  = function(parameters){
  gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(pH) + s(sulphates) + s(alcohol), data = d_train_light, family = gaussian(link = "identity"), sp = parameters)
  AIC(gamobj)
}
  
#And now we perform the grid search in parallel
split = detectCores() - 1  #detects the number of cores
cl = makeCluster(split) #creates the parallel environment
init = clusterEvalQ(cl, { library(mgcv); NULL }) #sets up the environment
clusterExport(cl, "d_train_light")  #exports the dataset to the other clusters

AICs = parLapplyLB(
  cl,   #The cluster element
  sp.list,    #The list where we apply the function
  fun  #The function applied to the list
)
# Find the minimum
opt.sp_light = sp.grid[which.min(AICs),]
print(opt.sp_light)
stopCluster(cl)
```


3. For $D_{train}^{clean}$
```{r}
sp1_clean = gam_clean$sp
sp2_clean = 2 * sp1_clean
t = rbind(sp1_clean,sp2_clean)

#we create the dataframe sp.grid whose rows are the different possible combinations of smoothing parameters
sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10], t[,11])
n.tuning = nrow(sp.grid)

#We create a list of the rows of sp.grid 
sp.list = split(sp.grid, seq(n.tuning))

#Now we create the function that for each vector of parameters returns the corresponding AIC of the model.
fun  = function(parameters){
  gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_train_clean, family = gaussian(link = "identity"), sp = parameters)
  AIC(gamobj)
}
  
#And now we perform the grid search in parallel
split = detectCores() - 1  #detects the number of cores
cl = makeCluster(split) #creates the parallel environment
init = clusterEvalQ(cl, { library(mgcv); NULL }) #sets up the environment
clusterExport(cl, "d_train_clean")  #exports the dataset to the other clusters

AICs = parLapplyLB(
  cl,   #The cluster element
  sp.list,    #The list where we apply the function
  fun  #The function applied to the list
)
# Find the minimum
opt.sp_clean = sp.grid[which.min(AICs),]
print(opt.sp_clean)
stopCluster(cl)
```


4. For $D_{train}^{light,clean}$
```{r}
sp1_light_clean = gam_light_clean$sp
sp2_light_clean = 2 * sp1_light_clean
t = rbind(sp1_light_clean,sp2_light_clean)

#we create the dataframe sp.grid whose rows are the different possible combinations of smoothing parameters
sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10])
n.tuning = nrow(sp.grid)

#We create a list of the rows of sp.grid 
sp.list = split(sp.grid, seq(n.tuning))

#Now we create the function that for each vector of parameters returns the corresponding AIC of the model.
fun  = function(parameters){
  gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(pH) + s(sulphates) + s(alcohol), data = d_train_light_clean, family = gaussian(link = "identity"), sp = parameters)
  AIC(gamobj)
}
  
#And now we perform the grid search in parallel
split = detectCores() - 1  #detects the number of cores
cl = makeCluster(split) #creates the parallel environment
init = clusterEvalQ(cl, { library(mgcv); NULL }) #sets up the environment
clusterExport(cl, "d_train_light_clean")  #exports the dataset to the other clusters

AICs = parLapplyLB(
  cl,   #The cluster element
  sp.list,    #The list where we apply the function
  fun  #The function applied to the list
)
# Find the minimum
opt.sp_light_clean = sp.grid[which.min(AICs),]
print(opt.sp_light_clean)
stopCluster(cl)
```






$$\text{aggiungi l'analisi dei risultati}$$
$$\text{aggiungi il fit sul test set}$$