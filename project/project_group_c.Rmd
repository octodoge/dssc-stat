---
title: "Final Project Group C"
author: "G. Alessio, A. Campagnolo, M. Polo, G. Sarnelli"
date: "2023-01-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project aim

The goal is to model wine quality based on physicochemical tests. 

# Data Exploration

In order to obtain better results with our models we explore the original dataset coming from the following source:

```{r}
#load dataset
d_tot = read.csv("winequality-white.csv", sep=";")
```

In order to look at the data we decide to plot 12 histograms for each input features $x^{(i)}_j$ where $j=1,\dots,11$, plus $y^{(i)}=\text{"the wine-quality"}$ that is our response variable.

```{r echo=FALSE}
library(ggplot2)
library(gridExtra)

plot_feature_hist <- function(data, name, color) {
  #bw <- 2 * IQR(d_tot[,col]) / length(d_tot[,col])^(1/3) # Freedman-Diaconisâ€™s Rule
  return(
    p <-
      ggplot(as.data.frame(data) , aes(x=data)) +
        geom_histogram(fill=color, color="#e9ecef", alpha=0.9) +
        ggtitle(name)
    )
}

cols <- names(d_tot)
hist_colors <- c("#003f5c", "#376a35", "#2f4b7c", "#89b8ba", "#689546", "#665191", "#a05195", "#d45087", "#f95d6a", "#ff7c43", "#ffa600", "#ed3a2d")

myplot = list()
for (i in 1:12){
  myplot[[i]] <- plot_feature_hist(d_tot[,cols[i]],cols[i], hist_colors[i]) + theme(plot.margin=unit(c(1,0,0,0),"cm"))
}

suppressMessages(grid.arrange(myplot[[1]], myplot[[2]], myplot[[3]], myplot[[4]], myplot[[5]], myplot[[6]], ncol=3, nrow=2))
suppressMessages(grid.arrange(myplot[[7]], myplot[[8]], myplot[[9]], myplot[[10]], myplot[[11]], myplot[[12]],  ncol=3, nrow=2))
```

Coming from this preliminary look a the data most of features presents **long tail** of occurrences far from the "head" or central part of the distribution. 

Afterwards in order to address intercorrelations among two or more features in a multiple regression model (multicollinearity). We plot the Correlation Matrix:
```{r message=FALSE}
library(corrplot)
#plot the correlation matrix
corrplot(cor(d_tot), method = "shade")
```

While a moderate amount of multicollinearity is acceptable in a regression model, a higher multicollinearity can be a cause for concern. We can see that the **density** is highly correlated and it can be removed, as the information provided by these variables is **redundant**. 

# Models

## Datasets

In order to assess the models we split the total dataset $D_{tot}$ in $D_{train} + D_{test} = D_{tot}$. We introduce the following datasets $D_{train}$, $D_{train}^{light}$, $D_{train}^{clean}$ and $D^{light,clean}_{train}$ used to obtain the models.

```{r}
#d_train (3918 rows)
set.seed(101)
train_indexes = sample(1:nrow(d_tot), size=nrow(d_tot)*0.8) #80%
d_train = d_tot[train_indexes,]
row.names(d_train) = NULL
x_train = subset(d_train, select=-quality)
y_train = d_train$quality

#d_test (980 rows)
d_test = d_tot[-train_indexes,]
row.names(d_test) = NULL
x_test = subset(d_test, select=-quality)
y_test = d_test$quality
```

1. $D_{train}$ is the dataset without any modification.

2. $D_{train}^{light}$ is the dataset obtained without the **density feature**, we remove it because the Correlation Matrix arises multicollinearity problem (more insights in the data exploration section) and in order to maintain consistency between the different models we decide to just look at it and remove the most correlated feature, as told the **density**. (*light phase*)

```{r}
#d_train_light (3918 rows)
d_train_light = subset(d_train, select=-density)
x_train_light = subset(x_train, select=-density)
y_train_light = y_train

#d_test_light (980 rows)
x_test_light = subset(x_test, select=-density)
y_test_light = y_test
```

3. $D_{train}^{clean}$ is the dataset obtained by excluding the observations outside the $[\overline{x}-4\sigma_{x_j},\overline{x}+4\sigma_{x_j}]$ where $x_j$ is the observation coming from the j-feature ($j=1,\dots,11$). (*clean phase*)

```{r}
#function to clean datasets
clean = function(dataset, x, k=4) { #4 standard deviations
  means = colMeans(x)
  sds = apply(x, MARGIN=2, FUN=sd)
  f = function(x) all(x>means-k*sds & x<means+k*sds)
  return (dataset[apply(x, MARGIN=1, FUN=f),])
}

#d_train_clean (3791 rows)
d_train_clean = clean(d_train, x=x_train)
x_train_clean = subset(d_train_clean, select=-quality)
y_train_clean = d_train_clean$quality
```

4.  $D_{train}^{light,clean}$ is the dataset obtained from $D_{train}^{light}$ by applying the *clean phase*. We decide to do the *light phase* before the *clean phase* because we don't want to remove observations coming from removing the **density feature**.

```{r}
#d_train_light_clean (still 3791 rows but could have been more)
d_train_light_clean = clean(d_train_light, x=x_train_light)
x_train_light_clean = subset(d_train_light_clean, select=-quality)
y_train_light_clean = d_train_light_clean$quality
```

Using this 4 datasets we decide to implement the following models: **GLM**, **GAM** and **RF**.

## General Linear Model (GLM)
Now we analyze the 4 different datasets in order to find a linear relationship between the features of the selected dataset and the response variable **quality**.

We combine the GLM (Generalized Linear Model) with the **LASSO method** (Least Absolute Shrinkage and Selection Operator). The LASSO method introduces a penalty term that is proportional to the sum of the absolute values of the coefficients, thus it forces the model to decrease or to remove the effect of some less important feature.

In order to avoid a too simple model considering only the main effects (11 features) we consider in addition their interactions $\binom{11}{2} = 55$.
Afterwards to properly use the LASSO method we need to rescale the features of our data, so that we can avoid an imbalance between coefficients. 

Re-scaling of the $D_{train}$
```{r}
#function to scale train sets
scale_train = function(x_train) {
  x_train_int = model.matrix(as.formula( ~ .*.), x_train)[,-1]
  values = scale(x_train_int)
  return (list(
    values=values,
    means=attr(values, which="scaled:center"),
    sds=attr(values, which="scaled:scale")))
}

# scaling
x_train_s = scale_train(x_train)
```

In order to choose the smoothing parameter $\lambda$ we use **10-fold cross validation**. For each $\lambda$ value in a grid, software fits the model to a part of the data and then checks the goodness of the predictions for the **quality** in the remaining data, using the **Mean Squared Error (MSE)**. We do this 10 times, then we using the approach called **one-standard-error rule** that is select a larger $\lambda$ that has a MSE (it's a random variable) one standard error above the minimum, which moves the estimates in the direction of a greater regularization (less likely to overfit the model and more interpretability). 

Fit using $D_{train}$ scaled
```{r message=FALSE, warning=FALSE}
library(glmnet)

#function to fit glm with lasso
fit_lasso <- function(x, y){
  cv <- cv.glmnet(x, y, family=gaussian, type.measure = "mse")
  return (glmnet(x, y, family=gaussian, lambda = cv$lambda.1se))
}

# fitting
set.seed(101)
lasso = fit_lasso(x_train_s$values, y_train)
#coef(lasso)
```


We apply the glm trained to the test set and compute the final **MSE**. 
```{r}
# prediction
x_test_int <- model.matrix(as.formula( ~ .*.), x_test)[,-1]
x_test_s <- scale(x_test_int, center=x_train_s$means, scale=x_train_s$sds)
paste('the MSE with d_train is',assess.glmnet(lasso, newx=x_test_s, newy=y_test)$mse[1])
```

We apply successively the same procedures for the remaining 3 dataset: $D_{train}^{light}$, $D_{train}^{clean}$ and $D^{light,clean}_{train}$. Therefore:

For $D_{train}^{light}$ we have the following results:
```{r message=FALSE, warning=FALSE}
# scaling
x_train_light_s <- scale_train(x_train_light)

# fitting
set.seed(101)
lasso_light = fit_lasso(x_train_light_s$values, y_train_light)
#coef(lasso_light)

# Prediction
x_test_light_int <- model.matrix(as.formula( ~ .*.), x_test_light)[,-1]
x_test_light_s <- scale(x_test_light_int, center=x_train_light_s$means, scale=x_train_light_s$sds)
paste('the MSE with d_train_light is',assess.glmnet(lasso_light, newx=x_test_light_s, newy=y_test_light)$mse[1])

```

For $D_{train}^{clean}$ we have the following results:
```{r message=FALSE, warning=FALSE}
# scaling
x_train_clean_s = scale_train(x_train_clean)

# fitting
set.seed(101)
lasso_clean = fit_lasso(x_train_clean_s$values, y_train_clean)
#coef(lasso_clean)

# Prediction
x_test_s2 <- scale(x_test_int, center=x_train_clean_s$means, scale=x_train_clean_s$sds)
paste('the MSE with d_train_clean is',assess.glmnet(lasso_clean, newx=x_test_s2, newy=y_test)$mse[1])
```

For $D_{train}^{light,clean}$ we have the following results:
```{r message=FALSE, warning=FALSE}
# scaling
x_train_light_clean_s = scale_train(x_train_light_clean)

# fitting
set.seed(101)
lasso_light_clean = fit_lasso(x_train_light_clean_s$values, y_train_light_clean)
#coef(lasso_light_clean)

# prediction
x_test_light_s2 <- scale(x_test_light_int, center=x_train_light_clean_s$means, scale=x_train_light_clean_s$sds)
paste('the MSE with d_train_light_clean is',assess.glmnet(lasso_light_clean, newx=x_test_light_s2, newy=y_test_light)$mse[1])

```

## General Additive Model (GAM)

#### Prediction

## Random Forest

- Fit using $D_{train}$

```{r}
library(randomForest) #v 4.6-14 for R 4.0.4
set.seed(101)

rf = randomForest(x_train, y_train, importance=TRUE)
mean((predict(rf, x_test) - y_test)^2) #mse
```

```{r}
varImpPlot(rf)
```

- Fit using $D_{train}^{light}$

```{r}
set.seed(101)

rf_light = randomForest(x_train_light, y_train_light, importance=TRUE)
mean((predict(rf_light, x_test_light) - y_test_light)^2) #mse
```

```{r}
varImpPlot(rf_light)
```

- Fit using $D_{train}^{clean}$

```{r}
set.seed(101)

rf_clean = randomForest(x_train_clean, y_train_clean, importance=TRUE)
mean((predict(rf_clean, x_test) - y_test)^2) #mse
```

```{r}
varImpPlot(rf_clean)
```

- Fit using $D_{train}^{light,clean}$

```{r}
set.seed(101)

rf_light_clean = randomForest(x_train_light_clean, y_train_light_clean, importance=TRUE)
mean((predict(rf_light_clean, x_test_light) - y_test_light)^2) #mse
```

```{r}
varImpPlot(rf_light_clean)
```

#### Prediction

- Application of the model using the $D_{test}$
- Assessing of the two different fit using MSE

# Conclusion

#### Comparison between fit using $D_{train}$

- 8 AIC comparison

#### Comparison between fit using $D_{test}$

- 10 MSE comparison

#### Improvements and limitations

- The models without the density gives quite the same MSE, as a demostration of its correlation with the features. Removing it can give a better interpretability. 
- 