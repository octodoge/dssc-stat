---
title: "Final Project Group C"
author: "G. Alessio, A. Campagnolo, M. Polo, G. Sarnelli"
date: "2023-01-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project aim

The goal is to model wine quality based on physicochemical tests. 

# Data Exploration

In order to obtain better results with our models we explore the original dataset coming from the following source:

```{r}

path  = 'winequality-white.csv'
data = read.csv(path, header = T, sep = ';')
```

In order to look at the data we decide to plot 12 histograms for each input features $x^{(i)}_j$ where $j=1,\dots,11$, plus $y^{(i)}=\text{"the wine-quality"}$ that is our response variable.

```{r echo=FALSE}
library(tidyverse)
library(hrbrthemes)
library(gridExtra)

plot_feature_hist <- function(data, col, name, color) {
  #bw <- 2 * IQR(data[,col]) / length(data[,col])^(1/3) # Freedman-Diaconisâ€™s Rule
  return(
    p <- data %>%
      ggplot( aes_string(x=col)) +
        geom_histogram(fill=color, color="#e9ecef", alpha=0.9) +
        ggtitle(name) +
        theme_ipsum() +
        theme(plot.title = element_text(size=12))
    )
}

cols <- names(data)
hist_colors <- c("#003f5c", "#376a35", "#2f4b7c", "#89b8ba", "#689546", "#665191", "#a05195", "#d45087", "#f95d6a", "#ff7c43", "#ffa600", "#ed3a2d")

myplot = list()
for (i in 1:12){
  myplot[[i]] <- plot_feature_hist(data, cols[i],cols[i], hist_colors[i]) + theme(plot.margin=unit(c(1,0,0,0),"cm"))
}

grid.arrange(myplot[[1]], myplot[[2]], myplot[[3]], myplot[[4]], myplot[[5]], myplot[[6]], myplot[[7]], myplot[[8]], ncol=4, nrow=2)
grid.arrange(myplot[[9]], myplot[[10]], myplot[[11]], myplot[[12]], ncol=2, nrow=2)
```

Coming from this preliminary look a the data most of features presents **long tail** of occurrences far from the "head" or central part of the distribution. 

Afterwards in order to address intercorrelations among two or more features in a multiple regression model (multicollinearity). We plot the Correlation Matrix:
```{r}
library(corrplot)
#plot the correlation matrix
corrplot(cor(data), method = "shade")
```

While a moderate amount of multicollinearity is acceptable in a regression model, a higher multicollinearity can be a cause for concern. We can see that the **density** is highly correlated and it can be removed, as the information provided by these variables is **redundant**. 
To prove that and to consider the possibility of removing the variable we look at the VIF coefficients:
```{r message=FALSE}
library(car)
model = lm(quality ~ ., data = data)
vif(model)
```
```{r}
hist(data$fixed.acidity, breaks = 100)
hist(data$volatile.acidity, breaks = 100)
hist(data$citric.acid, breaks = 100)
hist(data$residual.sugar, breaks = 100)
hist(data$chlorides, breaks = 100)
hist(data$free.sulfur.dioxide, breaks = 100)
hist(data$total.sulfur.dioxide, breaks = 100)
hist(data$density, breaks = 100)
hist(data$pH, breaks = 100)
hist(data$sulphates, breaks = 100)
hist(data$alcohol, breaks = 100)
hist(data$quality)
```

```{r}
#we obtain the sigmas and the mean of each column of features
sigmas = rep(NA,11)
means = rep(NA,11)
for (j in 1:11){
  sigmas[j] = sd(data[,j])
  means[j] = mean(data[,j])
}

#now we exclude the samples whose feature are outside the 4sigma intervals
lower = means - 3*sigmas
higher = means + 3*sigmas
print(cbind(lower,higher))
#we create the list "delete" that finds which rows are to delete in the dataframe
delete = rep(F, nrow(data))
#print(cbind(d[,1],higher[1], d[,1]>higher[1]))
for (j in 1:11){
  delete = (delete | data[,j]<lower[j] | data[,j]>higher[j])
}
print(paste("We removed:",sum(delete),"samples from our data"))  #how many observations are deleted
#now we delete the the suspected samples from the dataframe
d_clean = data[-(which(delete)),]
```

```{r}
#We can plot a lot of histograms to better visualize the data
hist(d_clean$fixed.acidity, breaks = 100)
hist(d_clean$volatile.acidity, breaks = 100)
hist(d_clean$citric.acid, breaks = 100)
hist(d_clean$residual.sugar, breaks = 100)
hist(d_clean$chlorides, breaks = 100)
hist(d_clean$free.sulfur.dioxide, breaks = 100)
hist(d_clean$total.sulfur.dioxide, breaks = 100)
hist(d_clean$density, breaks = 100)
hist(d_clean$pH, breaks = 100)
hist(d_clean$sulphates, breaks = 100)
hist(d_clean$alcohol, breaks = 100)
hist(d_clean$quality)
```

# Models

## Datasets

In order to assess the models we split the total dataset $D_{tot}$ in $D_{train} + D_{test} = D_{tot}$. We introduce the following datasets $D_{train}$, $D_{light}$, $D_{clean}$ and $D_{light,clean}$ used to obtain the models.

$$\text{codice dove caricare il dataset splittato}$$

1.  $D_{train}$ is the dataset without any modification.

2.  $D_{light}$ is the dataset obtained without the **density feature**, we remove it because the Correlation Matrix arises multicollinearity problem (more insights in the data exploration section) and in order to maintain consistency between the different models we decide to just look at it and remove the most correlated feature, as told the **density**. (*light phase*)

$$\text{codice su come ottenere il dataset}$$

3.  $D_{clean}$ is the dataset obtained by excluding the observations outside the $[\overline{x}-3\sigma_{x_j},\overline{x}+3\sigma_{x_j}]$ where $x_j$ is the observation coming from the i-feature ($j=1,\dots,11$). (*clean phase*)

$$\text{codice su come ottenere il dataset}$$

4.  $D_{light,clean}$ is the dataset obtained from $D_{light}$ by applying the *clean phase*. We decide to do the *light phase* before the *clean phase* because we don't want to remove observations coming from removing the **density feature**.

$$\text{codice su come ottenere il dataset}$$

```{r}
#load dataset
d = read.csv("winequality-white.csv", sep=";")

#d_test (979 rows)
set.seed(101)
test_indexes = sample(1:nrow(d), size=nrow(d)*0.2) #20%
d_test = d[test_indexes,]
row.names(d_test) <- NULL
x_test = subset(d_test, select=-quality)
y_test = d_test$quality

#d_test_light (979 rows)
x_test_light = subset(x_test, select=-density)
y_test_light = y_test

#d_train (3919 rows)
d_train = d[-test_indexes,]
row.names(d_train) <- NULL
x_train = subset(d_train, select=-quality)
y_train = d_train$quality

#d_train_light (3919 rows)
d_train_light = subset(d_train, select=-density)
x_train_light = subset(x_train, select=-density)
y_train_light = y_train

clean = function(dataset, train_set, k=3) { #3 standard deviations
  means = colMeans(train_set)
  sds = apply(train_set, MARGIN=2, FUN=sd)
  f = function(x) all(x>means-k*sds & x<means+k*sds)
  return (dataset[apply(train_set, MARGIN=1, FUN=f),])
}

#d_train_clean (3597 rows)
d_train_clean = clean(d_train, train_set=x_train)
x_train_clean = subset(d_train_clean, select=-quality)
y_train_clean = d_train_clean$quality

#d_train_light_clean (still 3597 rows but could have been more)
d_train_light_clean = clean(d_train_light, train_set=x_train_light)
x_train_light_clean = subset(d_train_light_clean, select=-quality)
y_train_light_clean = d_train_light_clean$quality
```

Using this 4 datasets we decide to implement the following models: **GLM**, **GAM** and **RF**.

## General Linear Model (GLM)

1. Fit using $D_{train}$ + LASSO

Using the train data


```{r echo=FALSE}
library(glmnet)

# glmlasso(data,aic){} for every alla the dataset
#data=data_train
fit1 = glm(quality ~ ., family = gaussian(link = "identity"), data = data)

#rescaling the data
data_scale = as.data.frame(scale(data[,1:11]))
quality = data$quality
data_scale = cbind(data_scale, quality)
rm(quality)

attach(data_scale)
x = cbind(fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol)
set.seed(4)  #we fix a seed for the random operations of cv
cv = cv.glmnet(x, quality, alpha=1, family=gaussian(link = "identity"), type.measure="mse", nfolds = 10) #we do a 10 fold CV with mean squared error loss function

lambda_min = cv$lambda.min#we find the best value of lambda
fit_lasso = glmnet(x, quality, alpha=1, family=gaussian(link = "identity"), lambda = lambda_min)
#fit_lasso = glmnet(x, quality, alpha=1, family=gaussian(link = "identity"))
print(paste("The AIC of the model is ",AIC(fit1)))
#print(paste("The AIC of the model with lasso is ",AIC(fit_lasso)))

```

2. Fit using $D_{light}$ + LASSO

Using the light data

```{r echo=FALSE}
fit1 = glm(quality ~ ., family = gaussian(link = "identity"), data = data)
print(paste("the AIC is ",AIC(fit1)))
```


3. Fit using $D_{clean}$ + LASSO

Using the clean data

```{r echo=FALSE}
fit1 = glm(quality ~ ., family = gaussian(link = "identity"), data = data)
print(paste("the AIC is ",AIC(fit1)))
```


4. Fit using $D_{light,clean}$ + LASSO

```{r echo=FALSE}
fit1 = glm(quality ~ ., family = gaussian(link = "identity"), data = data)
print(paste("the AIC is ",AIC(fit1)))
```

- Assessing of the four different fit using AIC

#### Prediction

-   Application of the model using the $D_{test}$
-   Assessing of the four different fit using MSE

## General Additive Model (GAM)

Now we try to use the GAM model to analyze the possibility of non-linear relationships between the features.

```{r}
library(mgcv)
#we fit the GAM model on the different datasets
#On d_train
gam_train = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_train, family = gaussian(link = "identity")) 

summary(gam_train)
plot(gam_train)
print(gam_train$aic)


#On d_train_clean
gam_clean = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_train_clean, family = gaussian(link = "identity")) 

summary(gam_clean)
plot(gam_clean)
print(gam_clean$aic)


#On d_train_light
gam_light = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(pH) + s(sulphates) + s(alcohol), data = d_train_light, family = gaussian(link = "identity")) 

summary(gam_light)
plot(gam_light)
print(gam_light$aic)


#On d_train_light_clean
gam_light_clean = gam( quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(pH) + s(sulphates) + s(alcohol), data = d_train_light_clean, family = gaussian(link = "identity")) 

summary(gam_light_clean)
plot(gam_light_clean)
print(gam_light_clean$aic)
```

Now we consider the different values of the smoothing parameters in the GAM and how we could improve them. We do this by checking if for each feature it would be better to use the already found smoothing parameter or its double.
```{r}
#For d_train
sp1_train = gam_train$sp
sp2_train = 2 * sp1_train
t = rbind(sp1_train,sp2_train) #creates a matrix with sp1 and sp2 as rows

sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10], t[,11]) # creates a dataframe whose rows are the different possible combinations of smoothing parameters
n.tuning = nrow(sp.grid)
#Now I create the vector of the different AIC for each combination of values
aic = rep(NA, n.tuning)
#And now I perform the grid search
for(i in 1:n.tuning){
 gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_train, family = gaussian(link = "identity"), sp = sp.grid[i,]) 
 aic[i] = AIC(gamobj)
 
}
# Find the minimum
opt.sp_train = sp.grid[which.min(aic),]
print(opt.sp_train) 
```

```{r}
#For d_train_clean
sp1_clean = gam_clean$sp
sp2_clean = 2 * sp1_clean
t = rbind(sp1_clean,sp2_clean) #creates a matrix with sp1 and sp2 as rows

sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10], t[,11]) # creates a dataframe whose rows are the different possible combinations of smoothing parameters
n.tuning = nrow(sp.grid)
#Now I create the vector of the different AIC for each combination of values
aic = rep(NA, n.tuning)
#And now I perform the grid search
for(i in 1:n.tuning){
 gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(density) + s(pH) + s(sulphates) + s(alcohol), data = d_train_clean, family = gaussian(link = "identity"), sp = sp.grid[i,]) 
 aic[i] = AIC(gamobj)
 
}
# Find the minimum
opt.sp_clean = sp.grid[which.min(aic),]
print(opt.sp_clean) 
```

```{r}
#For d_light
sp1_light = gam_light$sp
sp2_light = 2 * sp1_light
t = rbind(sp1_light,sp2_light) #creates a matrix with sp1 and sp2 as rows

sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10], t[,11]) # creates a dataframe whose rows are the different possible combinations of smoothing parameters
n.tuning = nrow(sp.grid)
#Now I create the vector of the different AIC for each combination of values
aic = rep(NA, n.tuning)
#And now I perform the grid search
for(i in 1:n.tuning){
 gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(pH) + s(sulphates) + s(alcohol), data = d_train_light, family = gaussian(link = "identity"), sp = sp.grid[i,]) 
 aic[i] = AIC(gamobj)
 
}
# Find the minimum
opt.sp_light = sp.grid[which.min(aic),]
print(opt.sp_light) 
```

```{r}
#For d_light_clean
sp1_light_clean = gam_light_clean$sp
sp2_light_clean = 2 * sp1_light_clean
t = rbind(sp1_light_clean,sp2_light_clean) #creates a matrix with sp1 and sp2 as rows

sp.grid = expand.grid(t[,1], t[,2], t[,3], t[,4], t[,5], t[,6], t[,7], t[,8], t[,9], t[,10], t[,11]) # creates a dataframe whose rows are the different possible combinations of smoothing parameters
n.tuning = nrow(sp.grid)
#Now I create the vector of the different AIC for each combination of values
aic = rep(NA, n.tuning)
#And now I perform the grid search
for(i in 1:n.tuning){
 gamobj = gam(quality ~ s(fixed.acidity) + s(volatile.acidity) + s(citric.acid) + s(residual.sugar) + s(chlorides) + s(total.sulfur.dioxide) + s(free.sulfur.dioxide) + s(pH) + s(sulphates) + s(alcohol), data = d_light_clean, family = gaussian(link = "identity"), sp = sp.grid[i,]) 
 aic[i] = AIC(gamobj)
 
}
# Find the minimum
opt.sp_light_clean = sp.grid[which.min(aic),]
print(opt.sp_light_clean) 
```

$$\text{aggiungi l'analisi dei risultati}$$
$$\text{aggiungi il fit sul test set}$$
## Random Forest

- Fit using $D_{train}$
- Fit using $D_{clean}$

#### Prediction

- Application of the model using the $D_{test}$
- Assessing of the two different fit using MSE

# Conclusion

#### Comparison between fit using $D_{train}$

- 8 AIC comparison

#### Comparison between fit using $D_{test}$

- 10 MSE comparison

#### Improvements and limitations