---
title: "GroupD_HW1"
author: "G. Marsich, M. Vicari, M. Polo, E. Malcapi"
date: "2022-10-20"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 1

### Ex 1.44

*Suppose the sample data distribution of* $\{y_i\} = \{y_1,...,y_n\}$ *is very highly skewed to the right, and we take logs and analyze* $\{x_i = log(y_i)\}$*.*

a. *Is* $\bar x = log(\bar y)$*? Why or why not?*
b. *Is* $median(\{x_i\}) = log[median(\{y_i\})]$? *Why or why not?*
c. *To summarize* $\{y_i\}$, we find $\bar x$ and then use $exp(\bar x)$*. Show that* $\begin{matrix} exp(\bar x) = (\prod_i y_i)^{1/n} \end{matrix}$*, called the geometric mean of* $\{y_i\}$*.*

**Solution**

**(a)** No, because:

$$
\bar x = \sum_i log(y_i) \ne log \left( \sum_i y_i \right) = log(\bar y)
$$

**(b)** No, because there is at least one counterexample:

```{r, echo=T}
y = rgamma(n=2, shape=2)
x = log(y)

median(x); log(median(y))
```

However, for larger $n$ they tend to be closer, and for odd $n$ they are equal because $log$ is a monotonically increasing function so it preserves the sample order.

**(c)** They are equal:

$$
exp(\bar x) = exp \left( {1\over n} \sum_i x_i \right) = exp \left( {1\over n} \sum_i log(y_i) \right) = \\
exp \left( \sum_i log(y_i^{1/n}) \right) = \prod_i exp[log(y_i^{1/n})] =
\left( \prod_i y_i \right)^{1/n}
$$

### Ex 1.48

*For a sample* $\{y_i\}$ *of size* $n$*,* $\begin{matrix} \sum_i \left| y_i-c \right| \end{matrix}$ *is minimized at* $c$ *= median. Explain why this property holds. (Hint: Starting at* $c$ *= median, what happens to* $\begin{matrix} \sum_i \left| y_i-c \right| \end{matrix}$ *as you move away from it in either direction?)*

**Solution**

Let the observations be ordered, let $c$ = median, let $a = c+\Delta$ and let $m$ be the number of observations smaller than $a$. <br>
If $\Delta > 0$ ("moving away from the median to the right") then $m \ge {n\over2}$. <br>
If $\Delta < 0$ ("moving away from the median to the left") then $m \le {n\over2}$. <br>
Let's demonstrate that $\begin{matrix} \sum_i \left| y_i-a \right| \ge \sum_i \left| y_i-c \right| \end{matrix}$:

$$
\sum_{i=1}^n \left| y_i-a \right| =
\sum_{i=1}^m [(c+\Delta)-y_i] + \sum_{i=m+1}^n [y_i - (c+\Delta)] = \\
m\Delta + \sum_{i=1}^m (c-y_i) - (n-m)\Delta + \sum_{i=m+1}^n (y_i-c) = \\
2m\Delta-n\Delta + \sum_{i=1}^n \left| y_i-c \right| \ge \sum_{i=1}^n \left| y_i-c \right|
$$

So $\begin{matrix} \sum_i \left| y_i-c \right| \end{matrix}$ is minimized at $c$ = median.

## FSDS - Chapter 2

### Ex 2.16

*Each day a hospital records the number of people who come to the emergency room for treatment.*

a. *In the first week, the observations from Sunday to Saturday are 10, 8, 14, 7, 21, 44, 60. Do you think that the Poisson distribution might describe the random variability of this phenomenon adequately. Why or why not?*
b. *Would you expect the Poisson distribution to better describe, or more poorly describe, the number of weekly admissions to the hospital for a rare disease? Why?*

**Solution**

**(a)** No, because the Poisson distribution has mean = variance, while our sample has very different values:
```{r, echo=T}
x = c(10, 8, 14, 7, 21, 44, 60)
mean(x); var(x)
```

**(b)** **TBD**

### Ex 2.27

*The distribution of* $X$ *= heights (cm) of women in the U.K. is approximately* $N(162,7^2)$*. Conditional on* $X = x$*, suppose* $Y$ *= weight (kg) has a* $N(3.0 + 0.40x, 8^2)$ *distribution. Simulate and plot 1000 observations from this approximate bivariate normal distribution. Approximate the marginal means and standard deviations for* $X$ *and* $Y$*. Approximate and interpret the correlation.*

**Solution**

Let's simulate and plot 1000 observations:

```{r, echo=T}
x = rnorm(n=1000, mean=162, sd=7)
y = rnorm(n=1000, mean=3+0.4*x, sd=8)
plot(x, y)
```

Let's approximate the marginal means and standard deviations:

```{r, echo=T}
mean(x); mean(y); sd(x); sd(y)
```

Let's approximate the correlation:

```{r, echo=T}
cor(x, y)
```
$X$ and $Y$ are slightly positively correlated, so when $X$ > mean(x) then typically $Y$ > mean(y), and when $X$ < mean(x) then typically $Y$ < mean(y).

### Ex 2.71

*When* $Y$ *has positively skewed distribution over the positive real line, statistical analyses often treat* $X = log(Y)$ *as having a* $N(\mu, \sigma^2)$ *distribution. Then* $Y$ *is said to have the log-normal distribution.*

a. *Derive an expression for the c.d.f.* $G$ *of* $Y$ *in terms of the c.d.f.* $F$ *of* $X$*, and take the derivative to obtain the p.d.f.* $g$ *of* $Y$*.*
b. *Use the information given in Exercise 2.66 about the m.g.f. of a normal random variable to show that* $E(Y) = e^{\mu + \sigma^2/2}$ *and* $var(Y) = [e^{\sigma^2} âˆ’ 1][E(Y)]^2$*. As shown for the gamma distribution in Exercise 2.45, the log-normal has standard deviation proportional to the mean.*
c. *Explain why the median of the distribution of* $Y$ *is* $e^\mu$*. What do the mean and median suggest about the skewness of the distribution?*
d. *For independent observations* $y_1,...,y_n$ *from the log-normal, we could summarize the distribution by finding* $\bar x$ *for* $\{x_i =log(y_i)\}$ *and then using* $exp(\bar x)$*. Show that* $\begin{matrix} exp(\bar x) = (\prod_i y_i)^{1/n} \end{matrix}$*, the geometric mean of* $\{y_i\}$*.*

**Solution**

**(a)** Let's express $G$ in terms of $F$:

$$
G(y) = Pr(Y \le y) = Pr[log(Y) \le log(y)] = Pr[X \le log(y)] = F[log(y)]
$$

Let's obtain $g$:

$$
g(y) = {d \over dy}G(y) = {d \over dy}F[log(y)] \overset{\text{chain rule}} = {1 \over y}f[log(y)]
$$

Since $f$ is the p.d.f of a normal distribution:

$$
g(y) = {1 \over y} \left( \frac{1}{\sigma \sqrt{2\pi}} exp \left[-{1 \over 2} \left( \frac{log(y) - \mu}{\sigma} \right)^2 \right] \right)
$$

**(b)** **TBD**

**(c)** **TBD**

**(c)** **TBD**

## CS - Chapter 1

### Ex 1.2

*Evaluate* $Pr(X < 0.5, Y < 0.5)$ *if* $X$ *and* $Y$ *have the following joint p.d.f.*
$$
f(x,y) = \begin{cases}
  x+3y^2/2 & 0<x<1 \;\&\; 0<y<1 \\
  0 & \text{otherwise}.
\end{cases}
$$

**Solution**

The probability corresponds to the volume under the portion of interest in the graph:

$$
\int_0^{1\over2} \int_0^{1\over2} x+{3\over2}y^2 \,dx\,dy =
\int_0^{1\over2} \left[ {x^2\over2} + {3\over2}y^2x \right]_0^{1\over2} \,dy =
\int_0^{1\over2} {1\over8}+{3\over4}y^2 \,dy =
\left[ {1\over8}y + {1\over4}y^3 \right]_0^{1\over2} = {3\over32}
$$

## CS - Chapter 3

### Ex 3.3

*Rewrite the following, replacing the loop with efficient code:*
```
n <- 100000; z <- rnorm(n)
zneg <- 0; j <- 1

for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}
```
*Confirm that your rewrite is faster but gives the same result.*

**Solution**

Let's time the original code:

```{r, echo=T}
n <- 100000; z <- rnorm(n)
zneg <- 0; j <- 1

system.time( #time the original code
  for (i in 1:n) {
    if (z[i]<0) {
      zneg[j] <- z[i]
      j <- j + 1
    }
  }
)
```

Let's time the rewritten code:

```{r, echo=T}
system.time(
  opt_zneg <- z[z<0] #rewritten code
)
```

Let's ensure both codes give the same result:

```{r, echo=T}
all(zneg == opt_zneg)
```

The rewrite is faster and gives the same result.
