---
title: "GroupD_HW1"
author: "G. Marsich, M. Vicari, M. Polo, E. Malcapi"
date: "2022-10-20"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 1

### Ex 1.41

*To investigate how *$\overline{y}$ *can vary from sample to sample of size n, for the simulation from  a bell-shaped population shown at the end of Section 1.5.3, take (a) 10,000 random  samples of size n = 30 each; (b) 10,000 random samples of size n = 1000 each. In  each case, form a histogram of the 10,000 *$\overline{y}$ *values and find their standard deviation.  Compare results and explain what this simulation reveals about the impact of sample  size on how study results can vary. (Chapter 3 shows that in sampling from a population  with standard deviation 16, the theoretical standard deviation of *$\overline{y}$ *values is* $16/\sqrt n$ *) *




**Solution**

we take 10.000 random samples of size n = 30 and n = 1000 both for a simulation a bell-shaped population (using rnorm) and we evaluate the mean in each iteration of all the random samples and store them (in the following code y1 for n = 30 size and y2 for n = 1000 size). 
Then we calculate the mean and standard deviation and compare them.

```{r ex 1.41, echo=TRUE}
randomSample <- 10000
y1 <- y2 <- rep(NA,randomSample)

for (i in 1:randomSample) {
  y1[i] <- mean(rnorm(30,100,16))
  y2[i] <- mean(rnorm(1000,100,16))
}
mean(y1);sd(y1);hist(y1)
mean(y2);sd(y2);hist(y2)
```

In the second case we can see that we have a smaller standard deviation so data are more clustered to the mean than the first case.

In the last part of the exercise we compare the theoretical formula (sampling from a population with standard deviation 16, the theoretical standard deviation of $\overline{y}$ values is $16/\sqrt n$)

```{r comparison with theoretical sd formula, echo=TRUE}
16/sqrt(30)
16/sqrt(1000)

```
they are the approximate same results of the previous standard deviations found


### Ex 1.43
*For a sample with mean $\bar y$, show that adding a constant $c$ to each observation changes the mean to $\bar y + c$, and the standard deviation s is unchanged. Show that multiplying each observation by $c$ changes the mean to $c \bar y$ and the standard deviation to $|c|s$.*


**Solution**

First we create a sample of 1000 elements with a p.d.f. $N(4,3)$, and evaluate the mean and standard deviation:
```{r, echo=T}
#Create the sample
b <- rnorm(n=1000,4,3)
#Evaluate the mean and standard deviation 
mean=mean(b)
s_deviation=sd(b)
```
Now we create a second sample by adding to each element of the first sample the costant $c=2$, and then we compare the mean and standard deviation of the two samples:
```{r, echo=T}
#Add c=2 to each element of the sample 
c=2
b1=b+c
#Compare the mean and standard deviation  
mean1=mean(b1)
s_deviation1=sd(b1)
cat("mean of the first sample +c: ", mean+c)
cat("mean of the second sample: ",mean1)
cat("standard deviation of the first sample: ",s_deviation)
cat("standard deviation of the second sample: ",s_deviation1)
```
Now we create the second sample by multiplying the elements of the first one by $c=2$, and compare again the mean and standard deviation:
```{r, echo=T}
#multiply the value by c=2
b1=c*b
#Compare the mean and variance  
mean1=mean(b1)
s_deviation1=sd(b1)
cat("mean of the first sample times c: ", mean*c)
cat("mean of the second sample: ",mean1)
cat("standard deviation of the first sample times c: ",s_deviation*c)
cat("standard deviation of the second sample: ",s_deviation1)

```


### Ex 1.44

*Suppose the sample data distribution of* $\{y_i\} = \{y_1,...,y_n\}$ *is very highly skewed to the right, and we take logs and analyze* $\{x_i = log(y_i)\}$*.*

a. *Is* $\bar x = log(\bar y)$*? Why or why not?*
b. *Is* $median(\{x_i\}) = log[median(\{y_i\})]$? *Why or why not?*
c. *To summarize* $\{y_i\}$, we find $\bar x$ and then use $exp(\bar x)$*. Show that* $\begin{matrix} exp(\bar x) = (\prod_i y_i)^{1/n} \end{matrix}$*, called the geometric mean of* $\{y_i\}$*.*

**Solution**

**(a)** <br>
No, because there is at least one counterexample:

```{r, echo=T}
y = rgamma(n=100, shape=2)
x = log(y)

mean(x); log(mean(y))
```

**(b)** <br>
No, because there is at least one counterexample:

```{r, echo=T}
y = rgamma(n=2, shape=2)
x = log(y)

median(x); log(median(y))
```

However, for larger $n$ they tend to be closer, and for odd $n$ they are equal because $log$ is a monotonically increasing function so it preserves the sample order.

**(c)** <br>
They are equal:

$$
exp(\bar x) = exp \left( {1\over n} \sum_i x_i \right) = exp \left( {1\over n} \sum_i log(y_i) \right) = \\
exp \left( \sum_i log(y_i^{1/n}) \right) = \prod_i exp[log(y_i^{1/n})] =
\left( \prod_i y_i \right)^{1/n}
$$


### Ex 1.45

*Find the Chebyshev inequality upper bound for the proportion of observations falling
at least (**a**) 1, (**b**) 2, (**c**) 3 standard deviations from the mean. Compare this to the
approximate proportions for a bell-shaped distribution. Why are the differences so large?*

**Solution**

Consider the Chebyshev inequality as seen during the lectures: given a r.v. $X$ such that $E(X^2) < \infty$ and a constant $a > 0$, then $$Pr(|X| ≥ a)≤ \frac{E(X^2)}{a^2}$$
Replacing in this formula $X$ with $X-\mu$ and $a$ with $\lambda \sigma$, knowing that by definition $E((X -\mu)^2) = \sigma^2$: $$Pr(|X-\mu| ≥ \lambda \sigma)≤ \frac{1}{\lambda^2}$$
To answer the questions proposed by the exercise, one should just replace, respectively, the value of $\lambda$ with $1$, $2$ and $3$.

a. $Pr(|X-\mu| ≥ \sigma)≤ 1$
  
   On the other hand, considering a r.v. $Y\sim N(0,1)$ we have that $$Pr(|Y| ≥ 1) = Pr(Y ≥ 1) + Pr(Y ≤ -1) = 1 - Pr(Y ≤ 1) + Pr(Y ≤ -1)$$ that can be computed as:
   ```{r, echo=T}
   1 - pnorm(1, 0, 1) + pnorm(-1, 0, 1)
   ```
   
   
b. $Pr(|X-\mu| ≥ 2 \sigma)≤ \frac{1}{4} = 0.25$

   On the other hand, considering a r.v. $Y\sim N(0,1)$ we have that $$Pr(|Y| ≥ 2) = Pr(Y ≥ 2) + Pr(Y ≤ -2) = 1 - Pr(Y ≤ 2) + Pr(Y ≤ -2)$$ that can be computed as:
   ```{r, echo=T}
   1 - pnorm(2, 0, 1) + pnorm(-2, 0, 1)
   ```
   
   
c. $Pr(|X-\mu| ≥ 3 \sigma)≤ \frac{1}{9} \approx 0.111$

   On the other hand, considering a r.v. $Y\sim N(0,1)$ we have that $$Pr(|Y| ≥ 3) = Pr(Y ≥ 3) + Pr(Y ≤ -3) = 1 - Pr(Y ≤ 3) + Pr(Y ≤ -3)$$ that can be computed as:
   ```{r, echo=T}
   1 - pnorm(3, 0, 1) + pnorm(-3, 0, 1)
   ```


I can observe that for all the three cases, the values computed for the stardard normal distribution are much smaller that the bounds given by the Chebyshev inequality. That may happen because the Chebyshev inequality appears as a very general formula.


### Ex 1.47

*The* least squares *property of the mean states that the data fall closer to $\overline{y}$ than to any other number $c$, in the sense that:* $$\sum_{i} (y_i - \overline{y})^2 < \sum_{i} (y_i - c)^2$$

*Prove this property by treating $f(c) = \sum_{i} (y_i - c)^2$ as a function of $c$ and deriving the value of $c$ that minimizes it.* 

**Solution**

Defining as suggested: $$f(c) = \sum_{i=1}^n (y_i - c)^2$$ one can observe that this results in a polynomial expression, so all derivatives of the function exist and are continuous. Therefore, to find the value that minimizes $c$: $$f'(c) = -2\sum_{i=1}^n (y_i - c)$$ $$f''(c) = 2n >0 $$
Setting $0 = f'(c) = -2\sum_{i=1}^n (y_i - c)$ we obtain: $$\sum_{i=1}^n y_i = \sum_{i=1}^n c \Longrightarrow \sum_{i=1}^n y_i = nc \Longrightarrow c=\overline{y}$$

Therefore, for $c=\overline{y}$ the function has a minimum, so for every other $c$ the function cannot have lower values than $f(\overline{y})$.




### Ex 1.48

*For a sample* $\{y_i\}$ *of size* $n$*,* $\begin{matrix} \sum_i \left| y_i-c \right| \end{matrix}$ *is minimized at* $c$ *= median. Explain why this property holds. (Hint: Starting at* $c$ *= median, what happens to* $\begin{matrix} \sum_i \left| y_i-c \right| \end{matrix}$ *as you move away from it in either direction?)*

**Solution**

Let the observations be ordered, let $c$ = median, let $a = c+\Delta$ and let $m$ be the number of observations smaller than $a$. <br>
If $\Delta > 0$ ("moving away from the median to the right") then $m \ge {n\over2}$; else if $\Delta < 0$ ("moving away from the median to the left") then $m \le {n\over2}$ (let's call these remarks "rem 1"). <br>
Let's demonstrate that $\begin{matrix} \sum_i \left| y_i-a \right| \ge \sum_i \left| y_i-c \right| \end{matrix}$:

$$
\sum_{i=1}^n \left| y_i-a \right| =
\sum_{i=1}^m [(c+\Delta)-y_i] + \sum_{i=m+1}^n [y_i - (c+\Delta)] = \\
m\Delta + \sum_{i=1}^m (c-y_i) - (n-m)\Delta + \sum_{i=m+1}^n (y_i-c) = \\
2m\Delta-n\Delta + \sum_{i=1}^n \left| y_i-c \right| \overset{\text{rem 1}}\ge \sum_{i=1}^n \left| y_i-c \right|
$$

So $\begin{matrix} \sum_i \left| y_i-c \right| \end{matrix}$ is minimized at $c$ = median.

## FSDS - Chapter 2

### Ex 2.3
*According to recent data at the FBI website, of all Blacks slain in the U.S., $85%$ are slain by Blacks, and of all Whites slain, $93%$ are slain by Whites. Let $Y = victim’s \; race$ and $X = offender’s \; race$.*

1. *Which conditional distribution do these probabilities refer to, $Y$ given $X$, or $X$ given $Y$ ?*
2. *Given that a murderer was White, what other probability do you need to estimate the conditional probability that the victim was White? To illustrate, fix a value of $0.60$ for that other probability and find the conditional probability.*


**Solution**


1. The conditional distribution we are referring is $P(X|Y)$ since we are considering the probability that $X=white\: or\: black$ *among the all population* of white or black victim $(Y)$
2. For the Bayes theorem we have that:
$$
P(Y=white|X=white)=\frac{P(Y=white)P(X=white|Y=white)}{P(X=white)}
$$
But from $\frac{P(X\cap Y_i)}{P(Y_i)}=P(X|Y_i)$ it follows that $P(X)=\sum_iP(Y_i)P(X|Y_i)$, so we can obtain: 
$$
P(Y=white|X=white)=\frac{P(Y=white)P(X=white|Y=white)}{P(Y=white)P(X=white|Y=white)+P(Y=black)P(X=white|Y=black)}
$$
Having the value of $P(Y=white)=0.6$ we obtain $P(Y=white|X=white)=0.90$

### Ex 2.5

*Suppose that a person is equally likely to be born on any of 365 days of the year.*  

*(a) For three people selected randomly, explain why the probability that they all have different  birthdays is [(365)(364)(363)]/365^3.*  

*(b) Show that if at least 23 people attend a social party, the probability exceeds 0.50 that at  least two people have the same birthday. State any further assumptions needed for your  solution. (The R function pbirthday(n) gives the probability that at least 2 of n people  have the same birthday.)* 

*(c) Use a simulation to show that if 50 people attend the party, the probability is 0.97 of at  least one common birthday. (If results seem counterintuitive, notice how the number of  pairs of attendees increases as the number of attendees n increases.)  *

**Solution**

a) that is because these are indipendent events. In the numerator, we multiply 365 for 364 because we do not want the second person to be born the same day of the first person and then for 363 because we do not want the third person to be born the same day than the others.

b)
The event of at least two of the n people having the same birthday is complementary to all n birthdays being different. 
Furthermore we assume there are no twins present in the group.

```{r part b ex 2.5, echo=TRUE}
o <- 365
pr <- 1/365
res <- 1
for (i in 1:23 ) {
  res = res * pr * o 
  cat(i, " -- ",1 - res)
  print(' ')
  o = o - 1
  
}
#same result with pbirthday function
#for (i in 1:50 ) {
#  cat(" - it",i, "with p:",1 - pbirthday(i))}
```


Probability that at least 2 out of 23 have the same birthday is 1 − 0.493 = 0.507.

c)

the following loop create 100000 times a sample of 50 observation of people's birthday attending a party (with the unique() function duplicates are removed so the variable ind is 1 everytime the sample has duplicates). At the end we divide ind with total number of samples to get the probability

```{r part c ex 2.5, echo=TRUE}
ind<-0;iter<- 100000
for(i in 1:iter){
  s<-sample(1:365,50,replace=TRUE);d<- unique(s)
  ind<-ind+as.numeric(length(d)!=length(s)) }
ind/iter
```

### Ex 2.7 

*For the simulation at the end of Section 2.3.1, explain why you could also simulate the mean  with a single binomial experiment of 30 million observations and probability 0.50 of a head for  each, dividing by 10,000,000. Do this and compare the result to the theoretical expected value.  *

**Solution**


```{r ex 2.7, echo=TRUE}
y <- rbinom(10000000,3,0.50) #the following simulate 10000000 times the no. of successes in 3 trials.
head(y, 10)
mean(y)
sd(y)

rbinom(1,30000000,0.50)/10000000 #this simulate what the exercise is asking to do.
```


Since events are independent and a random variable that has a binomial distribution is actually the sum of independent random variables that follow the Bernoulli distribution. Furthermore we can see that the values evaluated in both ways are the same as the theoretical expected value: 1.5000 (that is p * n = 0.5 * 3).


### Ex 2.10

*A method of statistical inference has probability 0.05 of yielding an incorrect result. How many independent times can the method be used until the probability of all the inferences being correct is less than 0.50?*

**Solution**

If there is a probability of 0.05 of yielding an incorrect result, then the probability to get a correct result in a trial is 0.95. Therefore, since the tests are independent, the probability to have had all correct results after the n-th trial is given by the product of the probabilities to get a correct result. Imposing it to be less than 0.50: $$(0.95)^n < 0.50 \Longrightarrow n>\log_{0.95}{0.50}$$ that is:

```{r, echo=T}
   log(0.50, 0.95)
```
So $n$ should be at least 14.






### Ex 2.16

*Each day a hospital records the number of people who come to the emergency room for treatment.*

a. *In the first week, the observations from Sunday to Saturday are 10, 8, 14, 7, 21, 44, 60. Do you think that the Poisson distribution might describe the random variability of this phenomenon adequately. Why or why not?*
b. *Would you expect the Poisson distribution to better describe, or more poorly describe, the number of weekly admissions to the hospital for a rare disease? Why?*

**Solution**

**(a)** <br>
No, because the Poisson distribution has mean = variance, while our sample has very different values:

```{r, echo=T}
x = c(10, 8, 14, 7, 21, 44, 60)
mean(x); var(x)
```

**(b)** <br>
The Poisson distribution assumes that the events occur independently and at a constant average rate. So we expect it to better describe weekly admissions for a rare disease than daily admissions for a generic treatment. In fact:

  * We expect rare diseases to occur more independently than other causes of hospitalization (eg.: car accidents may involve more people).
  * The average rate of hospitalization for a generic treatment may vary from day to day (eg.: workdays vs weekend days), invalidating one of the assumptions.

### Ex 2.19
*Lake Wobegon Junior College admits students only if they score above 400 on a standardized achievement test. Applicants from group A have a mean of 500 and a standard deviation of 100 on this test, and applicants from group B have a mean of 450 and a standard deviation of 100. Both distributions are approximately normal, and both groups have the same size.*
1. *Find the proportion not admitted for each group.*
2. *Of the students who are not admitted, what proportion are from group B?*
3. *A state legislator proposes that the college lower the cutoff point for admission to 300, thinking that the proportion of not-admitted students who are from group B would decrease. If this policy is implemented, determine the effect on the answer to (2)*


**Solution**
```{r, echo=T}
#Evaluating the proportion of not admitted students using the cumulative distribution function 
A_not_admitted=pnorm(400,mean=500,sd=100)
B_not_admitted=pnorm(400,mean=450,sd=100)
#This are the value of the proportion in % of the not admitted students 
A_not_admitted
B_not_admitted
#-------------------------------------------
#Since the total number of students in each group it's the same we can evaluate the 
#proportion of not admitted student of group B among all the non admitted student like this: 
B_not_admitted/(A_not_admitted+B_not_admitted)
#-----------------------------------------------------
#Here we show that decreasing the threshold of the exam 
#the proportion of non admitted student coming from group B increase 
A_not_admitted=pnorm(300,mean=500,sd=100)
B_not_admitted=pnorm(300,mean=450,sd=100)
B_not_admitted/(A_not_admitted+B_not_admitted)

```

### Ex 2.21

*Plot the gamma distribution by fixing the shape parameter k = 3 and setting the scale parameter  = 0.5, 1, 2, 3, 4, 5. What is the effect of increasing the scale parameter? (See also Exercise 2.48.)*

**Solution**

```{r ex 2.21, echo=TRUE}
xx <- seq (0,10, l = 1000)
plot(xx, dgamma(xx, shape = 3, scale = .5), xlab ="x", ylab ="f(x)", type ="l")
lines(xx, dgamma(xx, shape = 3, scale = 1), col = 2)
lines(xx, dgamma(xx, shape = 3, scale = 2), col = 3)
lines(xx,  dgamma(xx, shape = 3, scale = 3), col = 4)
lines(xx,  dgamma(xx, shape = 3, scale = 4), col = 5)
lines(xx,  dgamma(xx, shape = 3, scale = 5), col = 6)

```

Looking at the plot we can see that mean and standard deviation both increase (their values are proportional to  the scale $\theta$, that is equal to the inverse of the rate λ).

### Ex 2.27

*The distribution of* $X$ *= heights (cm) of women in the U.K. is approximately* $N(162,7^2)$*. Conditional on* $X = x$*, suppose* $Y$ *= weight (kg) has a* $N(3.0 + 0.40x, 8^2)$ *distribution. Simulate and plot 1000 observations from this approximate bivariate normal distribution. Approximate the marginal means and standard deviations for* $X$ *and* $Y$*. Approximate and interpret the correlation.*

**Solution**

Let's simulate and plot 1000 observations:

```{r, echo=T}
x = rnorm(n=1000, mean=162, sd=7)
y = rnorm(n=1000, mean=3+0.4*x, sd=8)
plot(x, y)
```

Let's approximate the marginal means and standard deviations:

```{r, echo=T}
mean(x); mean(y); sd(x); sd(y)
```

Let's approximate the correlation:

```{r, echo=T}
cor(x, y)
```

$X$ and $Y$ are slightly positively correlated, that is: when $X$ > mean(x) then typically $Y$ > mean(y), and when $X$ < mean(x) then typically $Y$ < mean(y).


### Ex 2.43

*Consider the exponential $pdf \ f(y; λ) = λe^{-λy}$ and $cdf \ F(y; λ) = 1 − e^{-λy}$, for $y≥0$.*

*a. Find the median*

*b. Find the lower quartile and the upper quartile.*

*c. Find $\mu$ by showing that it equals $\frac{1}{λ}$ times the integral of a gamma $pdf$. Explain why $\mu$ is greater than the median.*

*d. Find $σ$ by finding $E(Y^2)$ using a gamma $pdf$ and using expression (2.3).*

**Solution**

a. The median $M$ is defined as the quantile $x_{0.5}$, so: $$M = x_{0.5} = F^{-1}(0.5) = min (x_{0.5}|F(x_{0.5}) ≥ 0.5) \Longrightarrow 0.5 = 1 − e^{-λM} \Longrightarrow M = -\frac{\ln0.5}{\lambda}$$
   To get an approximate result:
   ```{r, echo=T}
   log(0.50)
   ```
   So it is, approximately: $M\approx\frac{0.693}{\lambda}$


b. To find lower and upper quartile one has to proceed quite similarly to point a.: $$x_{0.25} = F^{-1}(0.25) \Longrightarrow 0.25 = 1 − e^{-λx_{0.25}} \Longrightarrow x_{0.25} = -\frac{\ln0.75}{\lambda}$$
   To get an approximate result:
   ```{r, echo=T}
   log(0.75)
   ```
   So it is, approximately: $x_{0.25}\approx\frac{0.288}{\lambda}$

   $$x_{0.75} = F^{-1}(0.75) \Longrightarrow 0.75 = 1 − e^{-λx_{0.75}} \Longrightarrow x_{0.75} = -\frac{\ln0.25}{\lambda}$$
   To get an approximate result:
   ```{r, echo=T}
   log(0.25)
   ```
   So it is, approximately: $x_{0.75}\approx\frac{1.386}{\lambda}$
   
   
c. By definition, the expected value $E(Y) = \mu$ is defined as: $$E(Y) = \int_{0}^{+\infty} yλe^{-λy}\, dy \Longrightarrow E(Y)= \frac{1}{\lambda}\int_{0}^{+\infty} ze^{-z}\, dz \Longrightarrow E(Y) = \frac{1}{\lambda}$$
having changed the variable, with $z=\lambda y$, and used the integration by parts.

   By definition, $pdf$ and $cdf$ are related by this formula: $$F_{gamma}(x) = \int_{-\infty}^{x} f_{gamma}(y)\, dy \ , \ \ \lim_{x \to +\infty} F_{gamma}(x) = 1 \Longrightarrow \int_{-\infty}^{+\infty} f_{gamma}(y)\, dy = 1 \Longrightarrow E(Y) = \frac{1}{\lambda}\int_{-\infty}^{+\infty} f_{gamma}(y)\, dy$$
   Having a look at the graph of the $pdf$ of the exponential distribution:
   ```{r, echo=T}
   plot(function(y) dexp(y, rate = 3), xlim = c(0, 5), ylab=" f(y) ", xlab="y", main = "Exponential distribution with λ = 3, pdf")
   ```
   
   one observes that the value of the function $f(y; \lambda)$ is relatively high for values close to zero but decreases very quickly until being, on the graph, confountable with the null value for $y ≥ 2$. We can somehow imagine that on the right outliers are described, since the probability of getting one of them is very low, while on the left (let's say from $0$ to $1$) there are the "real values". Since the median is less affected by outliers than the mean, it follows that the mean is greater (more "to the right") than the median.
  

d. The expression (2.3) states that: $var(Y) = \sigma^2 = E(Y^2) - \mu^2$. By definition: $$E(Y^2) = \int_{0}^{+\infty} y^2\lambda e^{-\lambda y}\, dy $$ Compute the gamma $pdf$ for $\alpha = 3$: $$f_{gamma}(x) = \frac{\lambda^3 x^2 e^{-\lambda x}}{\int_{0}^{+\infty} y^2 e^{-y}\, dy}$$ Using the integration by parts for two times we obtain that the integral to the denominator is equal to 2: $\int_{0}^{+\infty} y^2 e^{-y}\, dy = 2$. Therefore we can rewrite $E(Y^2)$ as: $$E(Y^2) = 2\frac{1}{\lambda^2}\int_{0}^{+\infty} f_{gamma}(x)\, dx$$ Being, as already seen: $\int_{0}^{+\infty} f_{gamma}(x) = \lim_{x \to +\infty} F_{gamma}(x) = 1$, it follows, thanks to expression (2.3): $$\sigma^2 = \frac{2}{\lambda^2} - \left (\frac{1}{\lambda}\right)^2 \Longrightarrow \sigma = \frac{1}{\lambda}$$
  


### Ex 2.52
*The pdf $f$ of a $N(\mu,\sigma)$ distribution can be derived from the standard normal pdf $\phi(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}\: with \:z\in(-\infty,\infty)$.*

1. *Show that the normal cdf $F$ relates to the standard normal cdf $\Phi$ by $F[y]=\Phi[\frac{y-\mu}{\sigma}]$.*

2. *From (1), show that $f(y)=\frac{1}{\sigma}\phi(\frac{y-\mu}{\sigma})$, and show that $f(y;\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{(y-\mu)^2}{2\sigma}}$.*


**Solution**


We have that $F(y)=\int^y_{-\infty}f(z)dz$ while $\Phi(y)=\int^y_{-\infty}\phi(z)dz$ so $\Phi[\frac{y-\mu}{\sigma}]=\int^{\frac{y-\mu}{\sigma}}_{-\infty}\phi(z) dz$ and  making a change of variable from $z$ to $z'=z\sigma+\mu$ we obtain
$$
\int^{\frac{y-\mu}{\sigma}}_{-\infty}\phi(z) dz=\int^y_{-\infty}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(z'-\mu)^2}{2\sigma^2}} dz'
$$
That is exactly $F[y]$. Furthermore from $F[y]=\int^y_{-\infty}f(z)dz=\Phi[\frac{y-\mu}{\sigma}]=\int^y_{-\infty}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(z'-\mu)^2}{2\sigma^2}} dz'=\int^y_{-\infty}\frac{1}{\sigma}\phi(\frac{z'-\mu}{\sigma}) dz'\: \forall y$ we can affirm that $f(y)=\frac{1}{\sigma}\phi(\frac{z-\mu}{\sigma})$.


### Ex 2.53

*If $Y$ is a standard normal random variable, with cdf $\Phi$, what is the probability distribution of $X=\Phi(Y)$? Illustrate by randomly generating a million standard normal random variables, applying the cdf function $\Phi$ to each, and plotting histograms of the (a) $y$ values, (b) $x$ values.*

**Solution**


I will suppose that the random variable $\Phi(Y)$, with $\Phi$ the cdf of the standard normal distribution, will have a distribution function that is the uniform distribution function since the cdf of a random variable evaluated on the random variable itself gives a random variable with a uniform distribution.

```{r, echo=T}
#Generating a sample of one million variables with standard normal distribution 
y=rnorm(1000000,mean=0,sd=1)
#Generating the sample of one million random variables x that correspond to the cdf evaluated on y 
x=pnorm(y,mean=0,sd=1)
#Comparing the histogram of the two samples
par(mfrow=c(1,2))
hist(y,freq = F)
curve(dnorm(x, mean=0, sd=1), add=TRUE, lty=1, col=2) 
hist(x,freq = F)
curve(dunif(x), add=TRUE, lty=1, col=2) 
```

### Ex 2.67

*For n observations {*$y_i$*}, let *$y_{(1)}$ *≤ *$y_{(2)}$ *≤ ⋯ ≤*$y_{(n)}$ *denote their ordered values, called order statistics. Let *$q_i$ *be the i/(n + 1) quantile of the standard normal distribution, for  i = 1,..., n. When {*$y_i$*} are a random sample from a normal distribution, the plot of the points  (*$q_{(1)}$ *, *$y_{(1)}$ *),..., (*$q_{(n)}$ *,*$y_{(n)}$ *) should approximately follow a straight line, more closely so when n is  large. This normal quantile plot is a special case of a quantile-quantile (Q-Q) plot. The  R appendix of this book presents details.*                                                                  
*(a) Randomly generate (i) n = 10, (ii) n = 100, (iii) n = 1000 observations from a N(0, 1)  distribution and construct the normal quantile plot each time, using software such as the  R functions rnorm and qqnorm. Note that as n increases the points cluster more tightly  along the line y = x, which you can add to the plot with command abline(0, 1).*                                                              
*(b) Randomly generate 1000 observations from a $N(100, 16^2)$ distribution of IQ’s and construct the normal quantile plot. What is the slope of the line approximating these points?  *                                                                          
*(c) Randomly generate 1000 observations from the (i) exponential distribution (2.2), (ii) uniform distribution over (0, 1), using software such as the R functions rexp and runif. Construct the normal quantile plot in each case. Explain how they reveal the non-normality  of the data. *

*(d) For case (ii) in (c), find appropriate uniform quantiles for which the Q-Q plot would be  approximately linear. Construct the plot.*

**Solution**

a)
```{r ex 2.67 a, echo=TRUE}     
Y10 <-rnorm(10)
Y100 <-rnorm(100)
Y1000 <-rnorm(1000)

qqnorm(Y10,col='blue',main='Y10~ N(0,1)'); abline(0,1)
qqnorm(Y100,col='blue',main='Y100~ N(0,1)'); abline(0,1)
qqnorm(Y1000,col='blue',main='Y1000~ N(0,1)'); abline(0,1)
```

as n increase points cluster more close to the y = x line

b)
```{r ex 2.67 b, echo=TRUE}
yb <- rnorm(1000, 100, 16)
qqnorm(yb)
abline(100,16)
```

we change the values of abline() function and we find out that the slope of points is about 16

c)

```{r ex 2.67 c part a, echo=TRUE}
ye <- rexp(1000)

qqnorm(ye,col='blue', main='Y3 ~ exp(1)') 
```

We observe that the points on the qqplot do not follow a line: as a matter of fact, if the distributions were the same the curve would have been a line with slope = 1.
We know that the sample quantiles are taken from an exponential distribution: for x<0 the exponential distribution is not defined then we observe that the curve in the qq plot lays on the horizontal line y = 0.


```{r ex 2.67 c part b, echo=TRUE}
yu <- runif(1000)

qqnorm(yu, col='blue', main='Y4 ~ uniform(0,1)')  
```

The observations are the same for the previous point; moreover we can say that the points n the qqplot with x greater then 1 lay on the horizontal line y = 1, since all the values on the uniform distribution are in the interval [0,1]. 

d) 

qunif gives the quantile function for the uniform distribution;
c(1:n)/(n+1) is the vector evaluated as in the text of the exercise 
"Let $q_i$ be the i~(n + 1) quantile of the standard normal distribution, for
i = 1. ...,n."

we sort the vector of values generated from runif()

we build the plot that compares sorted values of yu with quantiles found before

```{r ex 2.67 d, echo=TRUE}
n <- 1000
yu <- runif(n)
q=qunif(c(1:n)/(n+1))
yu=sort(yu)
plot(q,yu)

```

so for the uniform(0,1) the appropriate quantiles are $q_i$ = i/(n+1)


### Ex 2.69

*For a sequence of independent, identical binary trials, explain why the probability distribution for $Y =$ the number of successes before failure number $k$ occurs has probability function* $$f(y; k, π) = \binom{y+k-1}{y}π^y(1-π)^k$$ *This distribution, studied further in Section 7.5.2 for analyzing count data, is called the* $negative \ binomial \ distribution$.

**Solution**

The total amount of trials is $y+k$, $y$ being the successes and $k$ being the failures. Note that, by the text of the problem, the last trial must be part of the $k$ failures. The probability that this happens in a specific order (for example, first of all we have all the $y$ successes and the all the $k$ failures) is simply given by: $$π^y(1−π)^k$$ being the trials independent. However, the order is not important, so we have to consider all the possible permutations of failures and successes, having fixed the last trial as a failure. Therefore the number of permutations is: $$\binom{y+k-1}{y}$$ since there are $k-1$ failures that are "all the same" and $y$ successes that are "all the same" as well. To obtain the final formula, we just have to multiply the two results, getting: $$f(y; k, π) = \binom{y+k-1}{y}π^y(1-π)^k$$





### Ex 2.71

*When* $Y$ *has positively skewed distribution over the positive real line, statistical analyses often treat* $X = log(Y)$ *as having a* $N(\mu, \sigma^2)$ *distribution. Then* $Y$ *is said to have the log-normal distribution.*

a. *Derive an expression for the c.d.f.* $G$ *of* $Y$ *in terms of the c.d.f.* $F$ *of* $X$*, and take the derivative to obtain the p.d.f.* $g$ *of* $Y$*.*
b. *Use the information given in Exercise 2.66 about the m.g.f. of a normal random variable to show that* $E(Y) = e^{\mu + \sigma^2/2}$ *and* $var(Y) = [e^{\sigma^2} − 1][E(Y)]^2$*. As shown for the gamma distribution in Exercise 2.45, the log-normal has standard deviation proportional to the mean.*
c. *Explain why the median of the distribution of* $Y$ *is* $e^\mu$*. What do the mean and median suggest about the skewness of the distribution?*
d. *For independent observations* $y_1,...,y_n$ *from the log-normal, we could summarize the distribution by finding* $\bar x$ *for* $\{x_i =log(y_i)\}$ *and then using* $exp(\bar x)$*. Show that* $\begin{matrix} exp(\bar x) = (\prod_i y_i)^{1/n} \end{matrix}$*, the geometric mean of* $\{y_i\}$*.*

**Solution**

**(a)** <br>
Let's express $G$ in terms of $F$:

$$
G(y) = Pr(Y \le y) = Pr[log(Y) \le log(y)] = Pr[X \le log(y)] = F[log(y)]
$$

Let's obtain $g$:

$$
g(y) = {d \over dy}G(y) = {d \over dy}F[log(y)] \overset{\text{chain rule}}= {1 \over y}f[log(y)]
$$

Since $f$ is the p.d.f of a normal distribution:

$$
g(y) = {1 \over y} \left( \frac{1}{\sigma \sqrt{2\pi}} exp \left[-{1 \over 2} \left( \frac{log(y) - \mu}{\sigma} \right)^2 \right] \right)
$$

**(b)** <br>
The Exercise 2.66 says that the moment generating function m.g.f. of a normal random variable $X$ is:

$$
m(t) := E(e^{tX}) = e^{\mu t + \sigma^2 t^2/2}
$$

Let's show that $E(Y) = e^{\mu + \sigma^2/2}$:

$$
X = log(Y) \implies Y = e^X \implies E(Y) = E(e^X) \overset{\text{ex 2.66}}= e^{\mu + \sigma^2/2}
$$
Note that $E(Y)^2 = e^{\mu 2 + \sigma^2}$ (let's call this expression "expr 1"). <br>
Let's show that $var(Y) = [e^{\sigma^2} − 1][E(Y)]^2$:

$$
var(Y) := E(Y^2) - E(Y)^2 = E(e^{2X}) - E(Y)^2 \overset{\text{ex 2.66}}= e^{\mu 2 + \sigma^2 2} - E(Y)^2 = \\
e^{\mu 2 + \sigma^2} e^{\sigma^2} - E(Y)^2 \overset{\text{expr 1}}= E(Y)^2e^{\sigma^2} - E(Y)^2 = E(Y)^2(e^{\sigma^2} − 1)
$$

**(c)** <br>
Since $X$ is normally distributed, its median is equal to its mean $\mu$, so $Pr(X \le \mu) = 0.5$. <br>
Let's show why the median of the distribution of $Y$ is $e^\mu$:

$$
Pr(X \le \mu) = 0.5 \implies Pr(log(Y) \le \mu) = 0.5 \implies Pr(Y \le e^\mu) = 0.5
$$

The median of the distribution of $Y$ is smaller than its mean and this suggests that the distribution is positively skewed.

**(d)** <br>
They are equal:

$$
exp(\bar x) = exp \left( {1\over n} \sum_i x_i \right) = exp \left( {1\over n} \sum_i log(y_i) \right) = \\
exp \left( \sum_i log(y_i^{1/n}) \right) = \prod_i exp[log(y_i^{1/n})] =
\left( \prod_i y_i \right)^{1/n}
$$

## CS - Chapter 1

### Ex 1.1
*Exponential random variable, $X\ge0$, has pdf $f(x)=\lambda e^{-\lambda x}$.*
1. *Find the cdf and the quantile function for $X$.*
2. *Find $Pr(X<\lambda)$ and the median of $X$.*
3. *Find the mean and variance of $X$.*


**Solution**

1. The cdf is defined as $F(y)=\int_0^yf(x)dx$ and so we obtain $F(y)=\int_0^y\lambda e^{-\lambda x}dx=1-e^{-\lambda y}$. The quantile function is defined as $F^{-1}(p)$ and so we have $Q_p=\frac{-log(1-p)}{\lambda}$.
2. $Pr(X<\lambda)=F(\lambda)=1-e^{-\lambda^2}$, while the median is $Q_{0.5}=\frac{-log(0.5)}{\lambda}\simeq\frac{0.7}{\lambda}$. 
3. The mean is $E[X]=\int_0^\infty xf(x)dx=\int_0^\infty x\lambda e^{-\lambda x}dx$ and using integration by parts we obtain $E[X]=\frac{x}{\lambda}e^{-\lambda x}|_\infty^0+\int^0_\infty -\frac{e^{-\lambda x}}{\lambda}dx=\frac{1}{\lambda}$. For the variance we evaluate $var(X)=E[X^2]+E^2[X]$ and with the same procedure we obtain $var(X)=\frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}$


### Ex 1.2

*Evaluate* $Pr(X < 0.5, Y < 0.5)$ *if* $X$ *and* $Y$ *have the following joint p.d.f.*
$$
f(x,y) = \begin{cases}
  x+3y^2/2 & 0<x<1 \;\&\; 0<y<1 \\
  0 & \text{otherwise}.
\end{cases}
$$

**Solution**

The probability corresponds to the volume under the portion of interest in the graph:

$$
\int_0^{1\over2} \int_0^{1\over2} x+{3\over2}y^2 \,dx\,dy =
\int_0^{1\over2} \left[ {x^2\over2} + {3\over2}y^2x \right]_0^{1\over2} \,dy =
\int_0^{1\over2} {1\over8}+{3\over4}y^2 \,dy =
\left[ {1\over8}y + {1\over4}y^3 \right]_0^{1\over2} = {3\over32}
$$


### Ex 1.6

*Let $X$ and $Y$ be non-independent random variables, such that $var(X) = \sigma^2_x$, $var(Y) = \sigma^2_y$ and $cov(X, Y) = \sigma^2_{xy}$. Using the result from Section 1.6.2, find $var(X + Y)$ and $var(X − Y)$.*

**Solution**

The (particular) result mentioned in the text of the problem is: $$a^TZ ∼ N(a^T\mu, a^T\Sigma a)$$

a. Here we have that: $$a^T = \left (\begin{matrix} 1 & 1 \end{matrix} \right) $$ $$Z = \left (\begin{matrix} X \\ Y \end{matrix} \right ) $$ $$\mu = \left (\begin{matrix} \mu_x \\ \mu_y \end{matrix} \right )$$ $$\Sigma = \left (\begin{matrix} \sigma^2_{x} & \sigma^2_{xy} \\ \sigma^2_{xy} & \sigma^2_{y} \end{matrix} \right )$$

   Therefore, from the formula previously indicated: $$var(X+Y) = a^T\Sigma a = \left (\begin{matrix} 1 & 1 \end{matrix} \right) \left (\begin{matrix} \sigma^2_{x} & \sigma^2_{xy} \\ \sigma^2_{xy} & \sigma^2_{y} \end{matrix} \right )\left (\begin{matrix} 1 \\ 1 \end{matrix} \right ) = \sigma^2_{x} + 2\sigma^2_{xy}+ \sigma^2_{y}$$
   

b. Here we have that: $$a^T = \left (\begin{matrix} 1 & -1 \end{matrix} \right)$$ $$Z = \left (\begin{matrix} X \\ Y \end{matrix} \right )$$ $$\mu = \left (\begin{matrix} \mu_x \\ \mu_y \end{matrix} \right )$$ $$\Sigma = \left (\begin{matrix} \sigma^2_{x} & \sigma^2_{xy} \\ \sigma^2_{xy} & \sigma^2_{y} \end{matrix} \right )$$

   Therefore, from the formula previously indicated: $$var(X-Y) = a^T\Sigma a = \left (\begin{matrix} 1 & -1 \end{matrix} \right) \left (\begin{matrix} \sigma^2_{x} & \sigma^2_{xy} \\ \sigma^2_{xy} & \sigma^2_{y} \end{matrix} \right )\left (\begin{matrix} 1 \\ -1 \end{matrix} \right ) = \sigma^2_{x} - 2\sigma^2_{xy}+ \sigma^2_{y}$$




### Ex 1.8 

*If *$log(x) ∼ N(μ,σ^2)$*, find the p.d.f. of X.*

**Solution**

Given that $log(x) ∼ N(μ,σ^2)$

The cumulative distribution function of X is,

$$=F_x(X) $$
$$=Pr[X \leq x] $$

applying logarithm and operations that preserve inequality direction

$$=Pr[logX \leq log x] ; x > 0 $$
$$=Pr \left[ \frac{logX - \mu}{\sigma} \leq \frac{logx - \mu}{\sigma}\right] ; x > 0 $$
$$=\Phi \left( \frac{logx - \mu}{\sigma} \right) ; x > 0 $$
that is the probability of the cumulative function of the normal standard distribution

the pdf of X is
$$= f_x(x) $$
$$= \frac{d}{dx} F_x(X) $$

$$= \phi \left( \frac{log x - \mu}{\sigma} \right) \frac{1}{x\sigma} ; x > 0 $$
$$=\frac{1}{x\sigma \sqrt {2\pi}}e^{- \frac{1}{2} \left( \frac{log x - \mu}{\sigma}  \right)^2}  ; 0 < x < \infty $$
X follows log normal distribution

## CS - Chapter 3

### Ex 3.3

*Rewrite the following, replacing the loop with efficient code:*
```
n <- 100000; z <- rnorm(n)
zneg <- 0; j <- 1

for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}
```
*Confirm that your rewrite is faster but gives the same result.*

**Solution**

Let's time the original code:

```{r, echo=T}
n <- 100000; z <- rnorm(n)
zneg <- 0; j <- 1

system.time( #time the original code
  for (i in 1:n) {
    if (z[i]<0) {
      zneg[j] <- z[i]
      j <- j + 1
    }
  }
)
```

Let's time the rewritten code:

```{r, echo=T}
system.time(
  opt_zneg <- z[z<0] #rewritten code
)
```

Let's ensure both codes give the same result:

```{r, echo=T}
all(zneg == opt_zneg)
```

The rewrite is faster and gives the same result.



### Ex 3.5

*Consider solving the matrix equation $Ax = y$ for $x$, where $y$ is a known $n$ vector and $A$ is a known $n×n$ matrix. The formal solution to the problem is $x = A^{−1}y$, but it is possible to solve the equation directly, without actually forming $A^{-1}$. This question explores this direct solution. Read the help file for `solve` before trying it.*

a. *First create an $A$, $x$ and $y$ satisfying $Ax = y.$*
   ```{r, eval=FALSE}
   set.seed(0); n <- 1000
   A <- matrix(runif(n*n),n,n); x.true <- runif(n)
   y <- A%*%x.true
   ```
   
   The idea is to experiment with solving $Ax = y$ for $x$, but with a known truth to compare the answer to.


b. *Using `solve`, form the matrix $A^{−1}$ explicitly and then form $x_1 = A^{−1}y$. Note how long this takes. Also assess the mean absolute difference between `x1` and `x.true` (the approximate mean absolute ‘error’ in the solution).*

c. *Now use `solve` to directly solve for $x$ without forming $A^{−1}$. Note how long this takes and assess the mean absolute error of the result.*

d. *What do you conclude?*

**Solution**

Following the instructions given by the text of the problem:

a.
   ```{r, echo=T}
   set.seed(0); n <- 1000
   A <- matrix(runif(n*n),n,n); x.true <- runif(n)
   y <- A%*%x.true
   ```

b.
   ```{r, echo=T}
   now1 <- Sys.time() 
   I = diag(n)
   A1 = solve(A, I) #inverse matrix of A, using the definition of it
   x1 = A1%*%y #x found with 1° method
   print(Sys.time() - now1)
   e1 <- abs(x1 - x.true)
   sum1 <- sum(e1)
   MAE1 <- sum1/n
   print(MAE1)
   ```


c.
   ```{r, echo=T}
   now2 <- Sys.time()
   x2 = solve(A, y) #x found with 2° method
   print(Sys.time() - now2)
   e2 <- abs(x2 - x.true)
   sum2 <- sum(e2)
   MAE2 <- sum2/n
   print(MAE2)
   ```

d. We printed for both the two methods the time required to do the computation of $x$ and the mean absolute difference. One can observe that the method in point c. is better since both the time to compute the result and the error has a lower value than those in point b. We imagine that in particular the value of the error is due to the fact that in point b. there is a higher amount of computations to be done then errors add up. Regarding the time, in point b. operations require, overall, more computational effort than the ones in point c.



### Ex 3.6 

*The empirical cumulative distribution function for a set of measurements {*$x_i$ 
*: i = 1, . . ., n} is*
$$\hat{F}(x) = \frac{\#{x_i < x}}{n}$$
*where #{*$x_i$*< x} denotes ‘number of *$x_i$ *values less than x’. When answering the following, try to ensure that your code is commented, clearly structured, and tested. To test your code, generate random samples using rnorm, runif, etc.*

*a. Write an R function that takes an unordered vector of observations x and returns the values of the empirical c.d.f. for each value, in the order corresponding to the original x vector. See ?sort.int.*

*b. Modify your function to take an extra argument plot.cdf, that when TRUE will cause the empirical c.d.f. to be plotted as a step function over a suitable x range.*

**Solution**

a) 

The function is cdf(x) and takes in the set of observed values x.

Declare variable x_m denoting set of measured values.

First sort the elements of x_m using sort().
 
Then using while loop for set of observed values check for how many numbers of x_m’s is x_m
 
Print cdf of each element of x.


```{r ex 3.6 a, echo=TRUE}
#defining cdf function
cdf <- function(x){
  
  set.seed(123)
  
  n<- 1000 # number of samples in measurements
  x_m <- rnorm(n,0,1) #generate set of measurements
  x_m <- sort(x_m) #sort measurements
  cdf <- c() #initialize vector for cdf
  
  for (i in 1:length(x)){
    j <- 1 #initialize the variable to count #(x_m[j]<x[i])
    while((x_m[j]<x[i])&& (j-1)<length(x_m)){
      j <- j+1  #increase j in case x_m[j]<x[i]
    }
    cdf <- c(cdf, ((j-1)/length(x_m))) #add cdf of x[i]
  }
  
  #print cdf of each corresponding x
  print("Corresponding c.d.f.")
  print(cdf)
    
}
#end of function

#generate observations
n<- 20
m <- 0
x<- rnorm(n,m,1)
print("observed values")
print(x)

#calling the function
cdf(x)

```

b)

Add an argument plot.cdf which when TRUE the next process continues to get stepwise plot.

Sort elements of x and cdf using sort.

Using plot function with type ’s’ plot the step function of c.d.f. with respect to observed values.

```{r ex 3.6 b, echo=TRUE}
cdf <- function(x,plot.cdf){
  set.seed(123)
  n<- 1000 # number of samples
  x_m <- rnorm(n,0,1) #generate set of measurements
  x_m <- sort(x_m) #sort measurements
  cdf <- c() #initialize vector for cdf
  
  for (i in 1:length(x)){
    j <- 1
    while((x_m[j]<x[i])&& (j-1)<length(x_m)){
      j <- j+1      
    }
    cdf <- c(cdf, ((j-1)/length(x_m)))
  }
  
  #print cdf of each corresponding x
  print("Corresponding c.d.f.")
  print(cdf)
  
  
    
    #for plotting cdf in the form of steps
    if(plot.cdf == TRUE){
      
      x<-sort(x)
      cdf<- sort(cdf)
      plot(x,cdf,type='s',xlab="Observed values", ylab = "c.d.f.")
    }
    
    
}

#generate observations
n<- 20
m <- 0
x<- rnorm(n,m,1)
print("observed values")
print(x)
plot.cdf <- TRUE

#calling the function
cdf(x,plot.cdf)

```